{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Thank you for contributing to TeachOpenCADD!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Set up your PR</b>: Please check out our <a href=\"https://github.com/volkamerlab/teachopencadd/issues/41\">issue</a> on how to set up a PR for new talktorials, including standard checks and TODOs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T04 Â· Predicting Drug Drug Interactions using SVM\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Vanessa Siegel, 2023, CADD Seminar, Centre for Bioinformatics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This talktorial introduces and explores the subject of drug-drug interactions (DDI) and their different types, paying special attention to the concepts of antagonism, additivity, and synergism. This will be followed by a closer look at Support Vector Machines (SVM) that use soft margin classifiers and then move towards a more detailed explanation on how to use a combined similarity matrix as a pairwise kernel function to solve the non-linear classification problem of predicting new DDI by comparing them to already known DDI.\n",
    "\n",
    "To build the combined similarity matrix, we will look at 2D and 3D structural similarity as well as similarity between interaction profiles and create databases for each of them. The dataset used during the practical part of this talktorial will be retrieved from [__DrugBank__](www.drugbank.ca) and filtered to only contain small molecule drugs that are annotated as approved for medical use in at least one country and for which 2D and 3D structural data is available.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "-\tDrug-Drug Interactions\n",
    "    - Importance of drug-drug interactions\n",
    "    - Drug-drug interaction types\n",
    "-\tDrug Similarity\n",
    "    - Defining drug similarity to a computer\n",
    "    - 2D similarity\n",
    "    - 3D similarity\n",
    "    - Interaction profile similarity\n",
    "-\tSupport Vector Machines\n",
    "    - Soft Margin Classifer\n",
    "    - Kernel Trick\n",
    "-\tDrugBank\n",
    "    - History\n",
    "    - Drug Entries\n",
    "-\tWorkflow: Similarity-Based SVM for DDI Prediction\n",
    "    - Feature Selection\n",
    "    - Data Selection\n",
    "    - Creating drug-drug similarity databases\n",
    "    - Creating drug-pair similarity matrices\n",
    "    - Creating the pairwise kernel\n",
    "    - Modelling and evaluating the SVM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "-\tReading input data from DrugBank\n",
    "-\tCreate a drug-drug 2D molecular structure similarity database using RDKit\n",
    "-\tCreate a drug-drug 3D pharmacophoric similarity database using the E3FP and RDKit\n",
    "-\tCreate a drug-drug interaction profile database using RDKit\n",
    "-\tConstruct a combined pairwise similarity matrix for the kernel function\n",
    "-\tModel and evaluate the SVM with Scikit-Learn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Similarity-based SVM predictor for DDI: [*J. Clin. Pharm. Ther.* (2018), __44(2)__, 268-275](https://pubmed.ncbi.nlm.nih.gov/30565313/)\n",
    "* Explanation of E3FP: [*J. Med. Chem.* (2017), __60__, 7393-7409](https://pubs.acs.org/doi/full/10.1021/acs.jmedchem.7b00696)\n",
    "* DrugBank: [*Nucleic Acids Res.* (2017), __8__](https://pubmed.ncbi.nlm.nih.gov/29126136/)\n",
    "* Fingerprints and Drug Similarity: [__Talktorial 004__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T004_compound_similarity/talktorial.ipynb)\n",
    "* RDKit package [__documentation__](https://www.rdkit.org/docs/index.html)\n",
    "* E3FP package [__documentation__](https://e3fp.readthedocs.io/en/latest/index.html)\n",
    "* Scikit-Learn package [__Documentation__](https://scikit-learn.org/stable/index.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug-Drug Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance of drug-drug interactions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical investigations and research in the biomedical field showed that to treat more complex diseases the administration of just one drug is often not enough. Diseases like HIV, cancer, or kidney failure, to name a few, often require a combination of drugs to achieve satisfactory results or improvements in the patients' health. However, the simultaneous use of multiple drugs often leads to the occurrence of drug-drug interactions (DDI).\n",
    "\n",
    "DDI are caused when one drug interfers with another in one or more stages of its lifetime circle in the body and through that influences the effectiveness of said drug. This means they either cause an unexpected medical effect or creates an unexpected but measurable difference of the two drugs in the patient's bloodstream. \n",
    "\n",
    "Notably, it does not matter if said influence or effect is beneficial or harmful to the biological system to be classified as a DDI. Thus, while DDI can be taken advantage of to increase a therapeutic effect, they are also a mayor cause for unwanted or unexpected adverse side effects in patients. Therefore, extensive knowledge about potential DDI is crucial in medical care.\n",
    "\n",
    "From ensuring that the patient doesn't take drugs which are known to negatively affect each other (e.g., causing adverse reactions, making one drug unusable, potentiating side effects to a severely harmful degree), over preventing existing medical conditions from worsening, up to taking advantage of synergistic DDI to give better treatment, knowledge of DDI has a wide field of applications. As such analysing new drugs or drug combinations for potential DDI is an important aspect in the advancement of personalized medicine.\n",
    "\n",
    "Since finding and verifying DDI is a costly process due to the amount of in vitro and in vivo experiments, computational means started to develop to filter the large number of potential drug combinations for those who show a high possibility for expressing DDI. Many of these computational methods use machine-learning techniques, such as the one discussed in this talktorial in more detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drug-drug interaction types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drug-drug interactions are commonly classified by the cause of their occurrence, meaning they are either caused by pharmacokinetic (PK) or pharmacodynamic (PD) interactions.\n",
    "\n",
    "__Pharmacokinetic DDI__ occur when drug A influences drug B's concentration in the blood stream. It does not matter if it is the active component of drug A or one of the additives, added to assist in the delivery of the drug, that is the cause of this interaction. \n",
    "PK DDI are separated into four categories depending on which stage of a drug's lifetime circle is affected: absorption, distribution, metabolism, or excretion (ADME).\n",
    "\n",
    "__Pharmacodynamic DDI__, on the other hand, occur when the pharmacological effect of drug A influences the pharmacological effect of drug B. This happens when A and B target similar or related biological pathways or targets. However, a PD DDI can also occur when the drugs affected pathways seem to be completely unrelated, but their pharmacological effects still cause an unexpected medical observation.\n",
    "\n",
    "PD DDI get classified into three groups: \n",
    "-\t__Antagonistic__: \n",
    "The combined effect caused by a drug combination is smaller than the sum of the pharmacological effects seen when each drug is given alone.\n",
    "-\t__Additive__:\n",
    "The combined effect caused by a drug combination is the sum of the pharmacological effects seen when each drug is given alone.\n",
    "-\t__Synergistic__:\n",
    "The combined effect caused by a drug combination is greater than the sum of the pharmacological effects seen when each drug is given alone.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison Pharmacodynamic DDI Types](./images/PD%20DDI.jpg)\n",
    "\n",
    "*Figure 1:* Graphical representation of the three types of pharmacodynamic drug-drug interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that a DDI can be of both types, signifying that this way of classification is to be seen more as a widely used guideline rather than hard-split categories. Similarly, the words antagonistic, additive, synergistic, and their synonyms are sometimes used to also categorize PK DDI, especially in the medical field where the distinction between PK and PD may not be of importance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__IMPORTANT__:\n",
    "\n",
    "For the remainder of this talktorial, unless specified otherwise, DDI will not be differentiated into PK or PD. Likewise, the terms antagonistic, additive, and synergistic â if applied â will be used to describe all DDI as smaller, equal, and greater than the sum of their __therapeutic effect__ respectively as to not distract from the actual topic of this talktorial.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Similarity\n",
    "\n",
    "A base assumption in drug discovery is that similar drugs express similar properties and thus behave similar when introduced to a biological system. Computer-based methods use that assumption to cluster drugs or find molecules that have the potential to have an increased therapeutic effect to already known drugs. As such, it is important to clearly define how *similarity* is to be judged computationally.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining drug similarity to a computer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple different categories and properties that can be used to define similarity between different drugs, and machine learning algorithms often use a combination of them to make better predictions. This process is called feature selection and can have significant impact on the reliability of a model. Similarly impactful is how eaxctly we compare the chosen properties, meaning which feature encoding methods we employ during the calculations of drug similarity. But how does a computer compare two drugs?\n",
    "\n",
    "The most common way is to use fingerprints for the different properties and calculate similarity between them with the Tanimoto coefficient as described in Talktorial T004.\n",
    "\n",
    "For the Tanimoto coefficient we will use the following formula during this talktorial:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ TC(A,B) = |A \\cap B| / |A \\cup B| $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown the formula divides the intersection of fingerprints A and B with the number of features present in the union of both fingerprints and calculates the *similarity* as a float value between 0 and 1 with 0 representing no similarity at all and 1 indicating the two drugs are *identical* in the given property."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Schematic example of 2D structural similarity calculation](./images/Fingerprint-based-molecular-similarity-approache-Modified.png)\n",
    "\n",
    "*Figure 2:*\n",
    "Example for the comparison between fingerprints of two molecules and the resulting Tanimoto coefficient. \n",
    "\n",
    "(Modified figure. Original taken from [*Molecular informatics.* (2021), __40__](https://www.researchgate.net/publication/351084895_Differential_Consistency_Analysis_Which_Similarity_Measures_can_be_Applied_in_Drug_Discovery))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D structural similarity\n",
    "For 2D structural similarity, we will create MACCS fingerprints and use the Tanimoto coefficient to calculate our 2D similarity score. A more detailed explanation of MACCS fingerprints can be found in [__Talktorial T004__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T004_compound_similarity/talktorial.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D structural similarity\n",
    "\n",
    "For the calculation of the 3D structural similarity, we will use the [__Extended 3-Dimensional FingerPirnt__](https://e3fp.readthedocs.io/en/latest/index.html) (E3FP) package developped and provided by Keiser Lab at University of California San Francisco.\n",
    "\n",
    "This package calculates 3D fingerprints by applying the logic behind extended connectivity fingerprints (ECFP) to a 3-dimensional space. E3FP represent, similar to MACCS fingerprints, the absence or presence of structural fragments within a molecule. These fragments retain their 3-dimensional orientation, meaning that while two fragments would be identical in the 2-dimensional space, the spacial orientation of the molecular bonds will be taken into account and declare them different enough to warrant seperate representations within the E3FP.\n",
    "\n",
    "How the E3FP package calculates said fingerprints is explained in more detail in the corresponding [__research document__](https://pubs.acs.org/doi/full/10.1021/acs.jmedchem.7b00696) and will not be discussed here any further. Likewise the [__documentation__](https://e3fp.readthedocs.io/en/latest/index.html) of the package offers further inside into the used functions if necessary.\n",
    "\n",
    "Once we have the E3FP we will then convert them into fingerprints as recognized by RDkit before using them to calculate the Tanimoto Coefficients for our 3D similarity matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction profile similarity\n",
    "\n",
    "Like 2D structural similarity, we will use the fingerprint method here as well and calculate the Tanimoto coefficient. For this, our vector will be the size of the number of drugs in our database, each position filled with a 1 if the drug has a known DDI with the drug corresponding to the respective cell and 0 otherwise.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are supervised learning models, used to analyse data for classification and regression purposes. Initially developed by Vladimir Vapnik and colleagues during the 1990s, SVMs became more and more popular as one of the most robust prediction methods in machine learning.\n",
    "\n",
    "Using a set of training examples and a binary label system, the SVM training algorithm maps each labelled training example to a point in space and then determines an optimal hyperplane to separate the two classes from each other. The hyperplane itself gets defined by a set of points from both classes, the so-called support vectors, and situated to maximize the margin, meaning the distance between itself and the chosen support vectors on both sides, to reliably separate the two classes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM schema](./images/optimal-hyperplane.png)\n",
    "\n",
    "*Figure 3:* A schematic example of an optimal separating hyperplane.\n",
    "Figure was taken from [Open Source Computer Vision](https://docs.opencv.org/4.x/d1/d73/tutorial_introduction_to_svm.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin Classifer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this talktorial, we will use a so-called __Soft Margin Classifier__, opposed to the stricter Hard Margin Classifier. This means that we allow both, outliers and potential misclassifications, in our training data when determining the position of the optimal separating hyperplane.\n",
    "\n",
    "That way, we guarantee that the positioning of the separating hyperplane is much more robust and less prone to overfit our training data, making the whole model more reliable when classifying new data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hard vs Soft Margin Classifier](./images/Hard-vs-Soft-Margin.jpg)\n",
    "\n",
    "*Figure 4:* A comparison between a Hard Margin Classifier (left) and a Soft Margin Classifier (right). Both optimal separating hyperplanes would be identical if the one blue outlier was not part of the data set. Figure was taken from [Mubaris NK](https://mubaris.com/posts/svm/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to do so, the error parameter C which is used to punish the presence of outliers and misclassifications must be carefully chosen as to neither overfit our model nor to reduce specificity to the point that predictions will mean nothing. It is a very important step in handling the Bias/Variance Trade-off typical for machine learning models and unique to Soft Margin Classifiers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Effect of C](./images/Effect-of-soft-margin-constant-C-Modified.png)\n",
    "\n",
    "*Figure 5:* Comparison of different values for error parameter C and their impact on determining the optimal separating hyperplane.\n",
    "\n",
    "(Figure was modified. Original from [*PLoS computational biology.* (2008), __4__](https://www.researchgate.net/publication/23442384_Support_Vector_Machines_and_Kernels_for_Computational_Biology))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Trick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably the figures above all show examples of linear classification problems, meaning the two classes can be easily separated from each other with a linear hyperplane. However, this does not work when faced with a classification problem as depicted in the figure below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNon-linear Classification Problem](./images/nonlinear%20data.png)\n",
    "\n",
    "*Figure 6:* An example of a non-linear classification problem.\n",
    "Figure was taken from [Andrea Perlato](https://www.andreaperlato.com/theorypost/introduction-to-support-vector-machine/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For problems like this, SVMs have to make use of the __kernel trick__ to change a non-linear classification problem into a linear one before finding the optimal separating hyperplane.\n",
    "\n",
    "To do so, SVMs take the data points and artificially move them into a higher dimension with the help of a __kernel function__ which plots the data points in a manner to make them linearly separable. In this higher dimension, the SVM will then find an optimal separating hyperplane as described. In the last step, said hyperplane will get transformed back into the original dimension where it may no longer be linear.\n",
    "\n",
    "A simple example of how the kernel trick works is detailed in Figure 7, where we artificially plot the datapoints from a one-dimensional space into a two-dimensional space with a polynomial kernel function, determine the hyperplane, and then transform the data back into the original space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kernel Trick for Non-Linear Classification Problem](./images/Kernel%20trick.png)\n",
    "\n",
    "*Figure 7:* An example of the kernel trick on one-dimensional drug-dosage data. The chosen kernel function is a polynomial function of degree two: $f(x) = dosage^2$.\n",
    "Figure was taken from [Andrea Perlato](https://www.andreaperlato.com/theorypost/introduction-to-support-vector-machine/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kernel function does not actually transform the data into a higher dimension and only calculates the relationships between every pair of points as if they were in the higher dimension, the whole method is called the kernel __trick__.\n",
    "\n",
    "As for the kernel function itself, there are many different ways of creating one. The kernel function introduced in this talktorial is a pairwise kernel method which uses a similarity matrix of drug pairs to perform the kernel trick. How exactly we calculate said function will be discussed in detail in the corresponding subsection of the *Workflow* chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DrugBank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__DrugBank__](https://go.drugbank.com/about) is a comprehensive, free-to-access, online database containing information on drugs and drug targets. First established in 2006 by Dr David Wishart's lab at the University of Alberta as a project to grant academic researchers easier access to detailed, structured information about drugs, DrugBank grew in size and popularity thanks to the backing of various research organisations as well as government funding. Now in its 5th version (version 5.1.10 as of 1st April 2023), Drug Bank contains over 15,000 drug entries with almost 5,296 non-redundant protein sequences being linked to them. Each entry contains more than 200 data fields and combines detailed drug data (chemical, pharmacological, pharmaceutical, etc.) with comprehensive drug target data like sequence, structure, and pathway, collected from bioinformatics and cheminformatics resources. [cited from https://go.drugbank.com/about]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drug Entries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in the DrugBank database is clearly structured, systemically ordered, and possesses a unique number through which each drug can be clearly identified and addressed within the database. The toolbar to the left provides an easy way to navigate through the different categories and subcategories for which information for the drug at hand is provided, allowing for intuitive and easy access to the sought after information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot of DrugBank Entry Aspirin](./images/DrugBank%20Aspirin.png)\n",
    "\n",
    "*Figure 8:* \n",
    "Screenshot of the DrugBank Entry of [__Aspirin__](https://go.drugbank.com/drugs/DB00945). The red-framed toolbar provides easy access to different categories of drug information provided by DrugBank. The blue square marks the unique accession number through which each drug can be identified within the database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to note that while categories for which no information is available might be removed from the navigation help, the presence of a certain field does not necessitate the presence of information. The example below nicely illustrates that while the category Pharmacology is present, for most of its subfields, information is not available."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot of DrugBank Entry 1,2-Benzodiazepin](./images/DrugBank%201%202-Benzodiazepine.png)\n",
    "\n",
    "*Figure 9:* \n",
    "Screenshot of the DrugBank Entry of [__1,2-Benzodiazepin__](https://go.drugbank.com/drugs/DB12537)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this talktorial the only relevant information needed from these entries are:\n",
    "\n",
    "-\tThe __DrugBank Accession Number__\n",
    "-\tThe __Type__ of the drug (Small Molecule or Biotech)\n",
    "-\tWhich __Group(s)__ the drug belongs to\n",
    "-\tThe 2D structure in the __SMILE__ format\n",
    "-\tThe 3D structure in the __3D-SDF file__ format\n",
    "-\tThe list of known __Drug Interactions__\n",
    "\n",
    "\n",
    "The DrugBank accession number will serve as an identification system during computations and consists out of the letters DB followed by a five-digit number.\n",
    "\n",
    "The Type and Group information is required to filter the database for only those drugs that qualify as small molecule drugs, and which were approved for use. \n",
    "\n",
    "The 2D and 3D structure information will be used to create fingerprints with RDkit and the E3FP package. \n",
    "\n",
    "Lastly the drug interactions will be used to create interaction profile fingerprints for each drug and also serve as the foundation for the assignment of labels to our training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__IMPORTANT__:\n",
    "\n",
    "It is to note that unlike the *Type*, the *Groups* are not an either-or classification system since DrugBank incorporates information from various countries and a drug that might be approved in one can still be illicit or experimental in another. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow: Similarity-Based SVM for DDI Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work-flow described here is for the most part taken from the paper Similarity-based SVM predictor for DDI. In this talktorial we use some simplifications of the algorithm to focus more on the underlying ideas rather than on tweaking the accuracy of the SVM to its best possible outcome. The paper can be found in the References and will not be cited again during the following paragraphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a very big part of machine learning algorithms, especially when handling biological data where there are lots of features but only a limited amount of datapoints to use. How one chooses features depends on the availability of the data, the impact said feature might have on the problem at hand, and the computational effort to use it.\n",
    "\n",
    "As such, the first thing we need to do is decide which features we want to focus on. As previously mentioned, three different features were chosen for the sake of this tutorial: 2D structure, 3D structure, and drug interactions.\n",
    "\n",
    "2D structure is a commonly used feature in drug discovery since it allows for relatively easy first comparisons between drugs and proved reliable enough to build the basis for QSAR methods in pharmacy and drug Discovery. Moreover, a 2D molecular representation is generally available for drugs.\n",
    "\n",
    "The 3D structure was chosen to take into account that spatial orientation plays a very important role in all forms of protein-ligand interactions, and just like 2D structure, 3D structure is often available as well.\n",
    "\n",
    "Since drugs who already share a list of drugs which they interact with are much more likely to also interact with the drugs for which experimental data only exists for one of them, drug interactions were chosen as a feature that is both closely related to the purpose of the SVM we build and has to be sufficiently available for our future training dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our features, it is time to collect our data. To predict DDI with a SVM, we require labelled datapoints for training with sufficiently enough examples for both classes to prevent too much bias towards one category.\n",
    "\n",
    "In our practical, we label drug pairs as having (1) or not having a DDI (0). For this example, neither the strength nor the type of DDI is of importance. If one wishes to pay more attention to a certain type of interaction, they need to adjust the labels accordingly.\n",
    "\n",
    "We took our data from the DrugBank database version 5.1.10 manually and stored the information in different files to be used later for the algorithm in the Practical part.\n",
    "\n",
    "The drugs we chose are all small molecule drugs that were approved for use in at least one country and which have SMILE strings and 3D-SDF files provided by DrugBank. If Drug Interactions are marked as 'Not Available' we will assume that these drugs do not show any noteworthy DDI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating drug-drug similarity databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dataset ready, it is now time to calculate similarity for each of the three features while looking at all possible drug combinations.\n",
    "\n",
    "To create the 2D structure similarity database we use RDkit to transform the SMILE representation of each drug into a MACCS fingerprint. Once we have these, we calculate the Tanimoto Coefficient for all possible combinations of drugs and store these values in a similarity matrix for easy access.\n",
    "\n",
    "For the 3D structure similarity database, we use, as mentioned before, the E3FP package to create fingerprints directly from the 3D-SDF files. Those will then be transformed into fingerprints as they are used by RDkit, before we calculate a similarity score for each possible drug combination.\n",
    "\n",
    "Lastly, to create an interaction profile database we first create fingerprints the size of our number of drugs and then transform them into RDkit fingerprints. Afterwards, we use the Tanimoto Coefficient to calculate the similarity between the interaction profiles between two drugs.\n",
    "\n",
    "All these fingerprints as well as the three final databases will be stored for quick check-ups during later calculations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating drug-pair similarity matrices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tables to look up similarity scores for each possible drug combination, it is time to build the drug-pair similarity matrices. The process is the same for all three features.\n",
    "\n",
    "Since the SVM treats pairs of drugs as singular instances, the similarity matrix needs to contain similarity scores between drug-pairs rather than individual drugs as is the case in the previously created databases.\n",
    "\n",
    "To calculate the entries in the similarity matrices for each feature we use the following formula:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S\\big((d_1,d_1') , (d_2,d_2')) = max \\big( s(d_1,d_2) \\cdot s(d_1',d_2') , s(d_2,d_1') \\cdot s(d_1, d_2'))$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The letter $s$ denotes the similarity scores previously calculated and stored in the above-mentioned databases, while $S$ stands for the similarity score of the drug-pairs and will be saved in the new similarity matrix.\n",
    "\n",
    "Although the values $s(d_1,d_1')$ and $s(d_1',d_1)$ are identical, there is a difference between $s(d_1,d_2) \\cdot s(d_1',d_2')$ and $s(d_1',d_2) \\cdot s(d_1,d_2')$. Therefore, we choose the maximum between those two values for $S$ to ensure we store the score for the comparison with maximum similarity between the individual drugs for better accuracy and predictability.\n",
    "\n",
    "Notably, the resulting drug-pair similarity matrices only contains the pair $(d_1,d_1')$ once and no cell or row marked with $(d_1',d_1)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the pairwise kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we simply add the scores of all three drug-pair similarity matrices together, giving each feature the same importance. As a result, the values in the final similarity matrix we use as our pairwise kernel function range from 0 to 3 rather than 0 to 1 as in the underlying drug-pair similarity matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling and Evaluating the SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build the actual model with the help of the Scikit-Learn package, we split our DDI instances into a training set of 90% and a testing set of 10% and adjust our kernel function accordingly to match our training data by removing the data rows and columns of our test set.\n",
    "\n",
    "Using our labelled training dataset and our new kernel function we create the SVM. Since we use a soft margin classifier, we add an error penalty parameter C with C>0 which is called the soft-margin constant. We chose four different values for $C$ in the range of {0.01, 0.1, 1.0, 10.0} and built consequently four different SVM.\n",
    "\n",
    "Afterwards, we predict the labels for our testing data and compare the predictions to the lables taken from DrugBank's Drug Interactions, counting how many predictions are 'correct' or 'false' and output the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will create drug similarity databases for 2D similarity, 3D similarity and interaction profile. Said databases will then be used to create a combined drug-pair similarity matrix shich will serve as the kernel function for a soft-margin-classifier SVM to predict DDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.lines import Line2D\n",
    "#import matplotlib.patches as mpatches\n",
    "\n",
    "# RDkit imports\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import (\n",
    "    Descriptors,\n",
    "    Draw,\n",
    "    PandasTools,\n",
    "    MACCSkeys,\n",
    "    rdFingerprintGenerator,\n",
    "    AllChem,\n",
    ")\n",
    "# E3FP package imports\n",
    "from e3fp.fingerprint.generate import (\n",
    "    fp,\n",
    "    fprints_dict_from_sdf,\n",
    ")\n",
    "\n",
    "# Scikit-Learn imports\n",
    "from sklearn import (\n",
    "    svm,\n",
    "    datasets,\n",
    "    metrics\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input data from DrugBank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data, which was retrieved from DrugBank's webserver, was saved in the form of an Excel Sheet. Each row is one drug entry, and each entry has four fields or columns. The columns are the following:\n",
    "\n",
    "* DrugBank Acession Number\n",
    "* SMILES\n",
    "* Name of the Drug\n",
    "* Comma-separated list of DrugBank Accession Numbers indicating DDI\n",
    "\n",
    "We use the pandas package to parse the Excel file into a dictionary with the fields *id*, *smiles*, *name*, and *ddi* to represent the four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = pd.read_excel(\"data/Input Data.xlsx\", header=None, names=[\"id\", \"smiles\", \"name\", \"ddi\"], index_col=None)\n",
    "\n",
    "# constants used throughout the tutorial\n",
    "num_drugs = drugs[\"id\"].size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a drug-drug 2D molecular structure similarity database using RDkit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function *create_2D_fp* which takes a SMILE and returns a MACCS fingerprint.\n",
    "\n",
    "To do so, our function first builds a molecule from the provided SMILE and then uses said molecule to calculate the MACCS fingerprint. Both will be done by the provided functions from the RDKit package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2D_fp(smile):\n",
    "    \"\"\"\n",
    "    takes a SMILE string and return a MACCS fingerprint\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "    return fp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting MACCS fingerprints will be stored as a new category in our *drugs* dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs[\"maccs\"] = [create_2D_fp(x) for x in drugs[\"smiles\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the fingerprints we create a similarity matrix containing the Tanimoto Scores calculated from the MACCS fingerprints.\n",
    "\n",
    "Since a similarity matrix is always symmetrical, we save calculation time by starting the inner loop at position i. That way we compare a drug only to those following after, since a comparison to the ones before were already done in the previous runs of the loop.\n",
    "\n",
    "For easier access to the scores saved within the similarity matrix, we create a panda DataFrame, naming the columns and rows after our drugs. That way, we can access the scores by using the names of the drugs rather than having to figure out at which specific index said drugs are within the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Tanimoto Coefficent of MACCS fingerprints\n",
    "scores_2D = np.zeros((num_drugs,num_drugs))\n",
    "for i in range(num_drugs):\n",
    "    for j in range(i, num_drugs):\n",
    "        score = DataStructs.TanimotoSimilarity(drugs[\"maccs\"][i],drugs[\"maccs\"][j])\n",
    "        scores_2D[i][j] = scores_2D[j][i] = score\n",
    "drugs_2D_database = pd.DataFrame(scores_2D, drugs[\"id\"], drugs[\"id\"])\n",
    "#print(drugs_2D_database)\n",
    "#print(drugs_2D_database[\"Amoxicilline\"][\"Furosemide\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a drug-drug 3D pharmacophoric similarity database using the E3FP and RDkit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step we do is build a function *create_3D_fp* which takes the path to a 3D-SDF file and creates an E3FP fingerprint with the help of the identically named package.\n",
    "\n",
    "For this, the E3FP package provides a helpful function *fprints_dict_from_sdf* which can calculate an E3FP fingerprint directly from the provided 3D-SDF file. This function takes the name of the SDF-file and an additional parameter *first=1* to define that we only want to look at the first conformer within the respective file.\n",
    "\n",
    "After that, we convert the E3FP fingerprint into a fingerprint as recognized by RDkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3D_fp(path):\n",
    "    \"\"\"\n",
    "    takes a 3D-SDF file and returns an E3FP fingerprint \n",
    "\n",
    "    params: path = str\n",
    "    path towards the 3D-SDF file\n",
    "    \"\"\"\n",
    "    fp_dict = fprints_dict_from_sdf(path, first=1)\n",
    "    fps = fp_dict[5][0]\n",
    "    fp = fps.fold().to_rdkit()\n",
    "    return fp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we create fingerprints for all our drugs and save them in our dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 00:49:01,696|INFO|Generating fingerprints for 2244.\n",
      "2023-07-03 00:49:01,721|INFO|Generated 1 fingerprints for 2244.\n",
      "2023-07-03 00:49:01,724|INFO|Generating fingerprints for 441300.\n",
      "2023-07-03 00:49:01,778|INFO|Generated 1 fingerprints for 441300.\n",
      "2023-07-03 00:49:01,782|INFO|Generating fingerprints for 71158.\n",
      "2023-07-03 00:49:01,803|INFO|Generated 1 fingerprints for 71158.\n",
      "2023-07-03 00:49:01,805|INFO|Generating fingerprints for 9811704.\n",
      "2023-07-03 00:49:01,957|INFO|Generated 1 fingerprints for 9811704.\n",
      "2023-07-03 00:49:01,960|INFO|Generating fingerprints for 1978.\n",
      "2023-07-03 00:49:02,023|INFO|Generated 1 fingerprints for 1978.\n",
      "2023-07-03 00:49:02,025|INFO|Generating fingerprints for 71771.\n",
      "2023-07-03 00:49:02,100|INFO|Generated 1 fingerprints for 71771.\n",
      "2023-07-03 00:49:02,103|INFO|Generating fingerprints for 1981.\n",
      "2023-07-03 00:49:02,216|INFO|Generated 1 fingerprints for 1981.\n",
      "2023-07-03 00:49:02,220|INFO|Generating fingerprints for 54676537.\n",
      "2023-07-03 00:49:02,288|INFO|Generated 1 fingerprints for 54676537.\n",
      "2023-07-03 00:49:02,291|INFO|Generating fingerprints for 6237.\n",
      "2023-07-03 00:49:02,348|INFO|Generated 1 fingerprints for 6237.\n",
      "2023-07-03 00:49:02,350|INFO|Generating fingerprints for 5904.\n",
      "2023-07-03 00:49:02,408|INFO|Generated 1 fingerprints for 5904.\n",
      "2023-07-03 00:49:02,411|INFO|Generating fingerprints for 2351.\n",
      "2023-07-03 00:49:02,477|INFO|Generated 1 fingerprints for 2351.\n",
      "2023-07-03 00:49:02,480|INFO|Generating fingerprints for 33613.\n",
      "2023-07-03 00:49:02,540|INFO|Generated 1 fingerprints for 33613.\n",
      "2023-07-03 00:49:02,543|INFO|Generating fingerprints for 3440.\n",
      "2023-07-03 00:49:02,596|INFO|Generated 1 fingerprints for 3440.\n",
      "2023-07-03 00:49:02,598|INFO|Generating fingerprints for 3639.\n",
      "2023-07-03 00:49:02,628|INFO|Generated 1 fingerprints for 3639.\n",
      "2023-07-03 00:49:02,631|INFO|Generating fingerprints for 5282379.\n",
      "2023-07-03 00:49:02,684|INFO|Generated 1 fingerprints for 5282379.\n",
      "2023-07-03 00:49:02,686|INFO|Generating fingerprints for 134664.\n",
      "2023-07-03 00:49:02,707|INFO|Generated 1 fingerprints for 134664.\n",
      "2023-07-03 00:49:02,709|INFO|Generating fingerprints for 3678.\n",
      "2023-07-03 00:49:02,759|INFO|Generated 1 fingerprints for 3678.\n",
      "2023-07-03 00:49:02,762|INFO|Generating fingerprints for 2170.\n",
      "2023-07-03 00:49:02,818|INFO|Generated 1 fingerprints for 2170.\n",
      "2023-07-03 00:49:02,820|INFO|Generating fingerprints for 9869929.\n",
      "2023-07-03 00:49:02,905|INFO|Generated 1 fingerprints for 9869929.\n",
      "2023-07-03 00:49:02,908|INFO|Generating fingerprints for 76314940.\n",
      "2023-07-03 00:49:02,985|INFO|Generated 1 fingerprints for 76314940.\n",
      "2023-07-03 00:49:02,987|INFO|Generating fingerprints for 2441.\n",
      "2023-07-03 00:49:03,034|INFO|Generated 1 fingerprints for 2441.\n",
      "2023-07-03 00:49:03,037|INFO|Generating fingerprints for 51263.\n",
      "2023-07-03 00:49:03,111|INFO|Generated 1 fingerprints for 51263.\n",
      "2023-07-03 00:49:03,114|INFO|Generating fingerprints for 5284371.\n",
      "2023-07-03 00:49:03,169|INFO|Generated 1 fingerprints for 5284371.\n",
      "2023-07-03 00:49:03,171|INFO|Generating fingerprints for 6726.\n",
      "2023-07-03 00:49:03,222|INFO|Generated 1 fingerprints for 6726.\n",
      "2023-07-03 00:49:03,225|INFO|Generating fingerprints for 34328.\n",
      "2023-07-03 00:49:03,304|INFO|Generated 1 fingerprints for 34328.\n",
      "2023-07-03 00:49:03,308|INFO|Generating fingerprints for 8223.\n",
      "2023-07-03 00:49:03,448|INFO|Generated 1 fingerprints for 8223.\n",
      "2023-07-03 00:49:03,451|INFO|Generating fingerprints for 213039.\n",
      "2023-07-03 00:49:03,544|INFO|Generated 1 fingerprints for 213039.\n",
      "2023-07-03 00:49:03,548|INFO|Generating fingerprints for 16678941.\n",
      "2023-07-03 00:49:03,658|INFO|Generated 1 fingerprints for 16678941.\n",
      "2023-07-03 00:49:03,663|INFO|Generating fingerprints for 2733526.\n",
      "2023-07-03 00:49:03,733|INFO|Generated 1 fingerprints for 2733526.\n",
      "2023-07-03 00:49:03,736|INFO|Generating fingerprints for 54676038.\n",
      "2023-07-03 00:49:03,798|INFO|Generated 1 fingerprints for 54676038.\n",
      "2023-07-03 00:49:03,801|INFO|Generating fingerprints for 176.\n",
      "2023-07-03 00:49:03,807|INFO|Generated 1 fingerprints for 176.\n",
      "2023-07-03 00:49:03,809|INFO|Generating fingerprints for 32170.\n",
      "2023-07-03 00:49:03,857|INFO|Generated 1 fingerprints for 32170.\n",
      "2023-07-03 00:49:03,859|INFO|Generating fingerprints for 2474.\n",
      "2023-07-03 00:49:03,917|INFO|Generated 1 fingerprints for 2474.\n",
      "2023-07-03 00:49:03,920|INFO|Generating fingerprints for 27125.\n",
      "2023-07-03 00:49:03,974|INFO|Generated 1 fingerprints for 27125.\n",
      "2023-07-03 00:49:03,977|INFO|Generating fingerprints for 3180.\n",
      "2023-07-03 00:49:04,029|INFO|Generated 1 fingerprints for 3180.\n",
      "2023-07-03 00:49:04,031|INFO|Generating fingerprints for 68942.\n",
      "2023-07-03 00:49:04,074|INFO|Generated 1 fingerprints for 68942.\n",
      "2023-07-03 00:49:04,077|INFO|Generating fingerprints for 996.\n",
      "2023-07-03 00:49:04,090|INFO|Generated 1 fingerprints for 996.\n",
      "2023-07-03 00:49:04,092|INFO|Generating fingerprints for 4914.\n",
      "2023-07-03 00:49:04,134|INFO|Generated 1 fingerprints for 4914.\n",
      "2023-07-03 00:49:04,137|INFO|Generating fingerprints for 5411.\n",
      "2023-07-03 00:49:04,182|INFO|Generated 1 fingerprints for 5411.\n",
      "2023-07-03 00:49:04,184|INFO|Generating fingerprints for 54678486.\n",
      "2023-07-03 00:49:04,242|INFO|Generated 1 fingerprints for 54678486.\n",
      "2023-07-03 00:49:04,245|INFO|Generating fingerprints for 1547484.\n",
      "2023-07-03 00:49:04,315|INFO|Generated 1 fingerprints for 1547484.\n",
      "2023-07-03 00:49:04,318|INFO|Generating fingerprints for 9651.\n",
      "2023-07-03 00:49:04,358|INFO|Generated 1 fingerprints for 9651.\n",
      "2023-07-03 00:49:04,361|INFO|Generating fingerprints for 3672.\n",
      "2023-07-03 00:49:04,396|INFO|Generated 1 fingerprints for 3672.\n",
      "2023-07-03 00:49:04,399|INFO|Generating fingerprints for 702.\n",
      "2023-07-03 00:49:04,404|INFO|Generated 1 fingerprints for 702.\n",
      "2023-07-03 00:49:04,406|INFO|Generating fingerprints for 64139.\n",
      "2023-07-03 00:49:04,457|INFO|Generated 1 fingerprints for 64139.\n",
      "2023-07-03 00:49:04,461|INFO|Generating fingerprints for 125017.\n",
      "2023-07-03 00:49:04,508|INFO|Generated 1 fingerprints for 125017.\n",
      "2023-07-03 00:49:04,511|INFO|Generating fingerprints for 5280795.\n",
      "2023-07-03 00:49:04,601|INFO|Generated 1 fingerprints for 5280795.\n",
      "2023-07-03 00:49:04,604|INFO|Generating fingerprints for 6917698.\n",
      "2023-07-03 00:49:04,704|INFO|Generated 1 fingerprints for 6917698.\n",
      "2023-07-03 00:49:04,706|INFO|Generating fingerprints for 3007.\n",
      "2023-07-03 00:49:04,725|INFO|Generated 1 fingerprints for 3007.\n",
      "2023-07-03 00:49:04,727|INFO|Generating fingerprints for 10026.\n",
      "2023-07-03 00:49:04,743|INFO|Generated 1 fingerprints for 10026.\n"
     ]
    }
   ],
   "source": [
    "drugs[\"e3fp\"] = [create_3D_fp(x) for x in \"data/\"+drugs[\"name\"]+\".sdf\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we calculate and save the Tanimoto Coefficients for the E3FP fingerprints in a similarity matrix in the same manner as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Tanimoto coefficient\n",
    "scores_3D = np.zeros((num_drugs,num_drugs))\n",
    "for i in range(num_drugs):\n",
    "    for j in range(i, num_drugs):\n",
    "        score = DataStructs.TanimotoSimilarity(drugs[\"e3fp\"][i],drugs[\"e3fp\"][j])\n",
    "        scores_3D[i][j] = scores_3D[j][i] = score\n",
    "drugs_3D_database = pd.DataFrame(scores_3D, drugs[\"id\"], drugs[\"id\"])\n",
    "#print(drugs_3D_database)\n",
    "#print(drugs_3D_database[\"Amoxicilline\"][\"Furosemide\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a drug-drug interaction profile database using RDkit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function *create_IP_fp* to calculate the interaction profile fingerprint of a single drug.\n",
    "\n",
    "Unlike in the above cases, instead of calling pre-existing functions, we have to create the interaction profile fingerprint from scratch. As described in the respetive Theory part, we create our own fingerprint-template and fill it with 0 and 1 accordingly.\n",
    "\n",
    "The fingerprint-template is defined by the parameter *y* which gives us a list of drugs in a predetermined order. The fingerprint, represented as a numpy array, will have the same length as *y* and each position in this array correlates to a drug at the same position in *y*. This ensures that all created fingerprints are uniform as long as *y* does not change.\n",
    "\n",
    "The resulting numpy array will then be converted into a bitstring and then a fingerprint as recognized by RDkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_IP_fp(x, y):\n",
    "    \"\"\"\n",
    "    Takes 2 lists of strings and creates an interaction profile fingerprint\n",
    "\n",
    "    params:\n",
    "    x = list of drugs with known DDI\n",
    "    y = list of drugs used as basis for the fingerprint\n",
    "    \"\"\"\n",
    "    DDI_fp = np.zeros(y.size)\n",
    "    for i in range(y.size):\n",
    "        if y[i] in x:\n",
    "            DDI_fp[i] = 1\n",
    "    # change array into bitstring\n",
    "    bitstring = \"\".join(DDI_fp.astype(str))\n",
    "    # change bitstring to RDkit fingerprint\n",
    "    fp = DataStructs.cDataStructs.CreateFromBitString(bitstring)\n",
    "    return fp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call this function for all elements within our drugs dataset and save the resulting fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs[\"ipfp\"] = [create_IP_fp(x,drugs[\"id\"]) for x in drugs[\"ddi\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we once again calculate and save the Tanimoto Coefficients in a similarity matrix as a pandas DataFrame with the names of the drugs to specify columns and rows of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Tanimoto Coefficient\n",
    "scores_IP = np.zeros((num_drugs,num_drugs))\n",
    "for i in range(num_drugs):\n",
    "    for j in range(i, num_drugs):\n",
    "        score = DataStructs.TanimotoSimilarity(drugs[\"ipfp\"][i],drugs[\"ipfp\"][j])\n",
    "        scores_IP[i][j] = scores_IP[j][i] = score\n",
    "drugs_IP_database = pd.DataFrame(scores_IP, drugs[\"id\"], drugs[\"id\"])\n",
    "#print(drugs_IP_database)\n",
    "#print(drugs_IP_database[\"Amoxicilline\"][\"Furosemide\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a combined pairwise similarity matrix for the kernel function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create our actual training-data points for the SVM, meaning we create instances of DDI (aka drug-pairs) and label them with 1 for an interaction and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labelled drug-pair data points\n",
    "drug_pairs_partner = []\n",
    "drug_pairs_names = []\n",
    "drug_pairs_labels = []\n",
    "for i in range(num_drugs):\n",
    "    for j in range(i+1, num_drugs):\n",
    "        drug_pairs_partner.append([drugs[\"id\"][i], drugs[\"id\"][j]])\n",
    "        drug_pairs_names.append(drugs[\"name\"][i] + \" + \" + drugs[\"name\"][j])\n",
    "        if drugs[\"id\"][j] in drugs[\"ddi\"][i] or drugs[\"id\"][i] in drugs[\"ddi\"][j]:\n",
    "            drug_pairs_labels.append(1)\n",
    "        else:\n",
    "            drug_pairs_labels.append(0)\n",
    "\n",
    "#print(drug_pairs_partner)\n",
    "#print(drug_pairs_names)\n",
    "#print(drug_pairs_labels)\n",
    "\n",
    "num_pairs = len(drug_pairs_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our drug-pairs as single instances, we have to calculate similarity matrices for 2D, 3D, and interaction profile similarity and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairwise similarity matrices\n",
    "pair_scores_2D = np.zeros((num_pairs,num_pairs))\n",
    "pair_scores_3D = np.zeros((num_pairs,num_pairs))\n",
    "pair_scores_IP = np.zeros((num_pairs,num_pairs))\n",
    "for i in range(num_pairs):\n",
    "    for j in range(i, num_pairs):\n",
    "        d1 = drug_pairs_partner[i]\n",
    "        d2 = drug_pairs_partner[j]\n",
    "        # calculate 2D pairwise similarity\n",
    "        score1 = np.dot(drugs_2D_database[d1[0]][d2[0]] , drugs_2D_database[d1[1]][d2[1]])\n",
    "        score2 = np.dot(drugs_2D_database[d1[1]][d2[0]] , drugs_2D_database[d1[0]][d2[1]])\n",
    "        pair_scores_2D[i][j] = pair_scores_2D[j][i] = max(score1, score2)\n",
    "        \n",
    "        #calculate 3D pairwise similarity\n",
    "        score1 = np.dot(drugs_3D_database[d1[0]][d2[0]] , drugs_3D_database[d1[1]][d2[1]])\n",
    "        score2 = np.dot(drugs_3D_database[d1[1]][d2[0]] , drugs_3D_database[d1[0]][d2[1]])\n",
    "        pair_scores_3D[i][j] = pair_scores_3D[j][i] = max(score1, score2)\n",
    "\n",
    "        #calculate IP pairwise similarity\n",
    "        score1 = np.dot(drugs_IP_database[d1[0]][d2[0]] , drugs_IP_database[d1[1]][d2[1]])\n",
    "        score2 = np.dot(drugs_IP_database[d1[1]][d2[0]] , drugs_IP_database[d1[0]][d2[1]])\n",
    "        pair_scores_IP[i][j] = pair_scores_IP[j][i] = max(score1, score2)\n",
    "\n",
    "# save pairwise similarity databases for quick checkups\n",
    "pair_2D_database = pd.DataFrame(pair_scores_2D, drug_pairs_names, drug_pairs_names)\n",
    "pair_3D_database = pd.DataFrame(pair_scores_3D, drug_pairs_names, drug_pairs_names)\n",
    "pair_IP_database = pd.DataFrame(pair_scores_IP, drug_pairs_names, drug_pairs_names)\n",
    "#print(pair_IP_database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to combine these three similarity matrices into one that will be used for the kernel function by performing a simple matrix addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_similarity_matrix = pair_2D_database + pair_3D_database + pair_IP_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspirin + Abacavir</th>\n",
       "      <th>Aspirin + Acamprosate</th>\n",
       "      <th>Aspirin + Acarbose</th>\n",
       "      <th>Aspirin + Acebutolol</th>\n",
       "      <th>Aspirin + Aceclofenac</th>\n",
       "      <th>Aspirin + Acemetacin</th>\n",
       "      <th>Aspirin + Acenocoumarol</th>\n",
       "      <th>Aspirin + Benziodarone</th>\n",
       "      <th>Aspirin + Benzylpenicillin</th>\n",
       "      <th>Aspirin + Bepridil</th>\n",
       "      <th>...</th>\n",
       "      <th>Desvenlafaxine + Cholecalciferol</th>\n",
       "      <th>Desvenlafaxine + Cisapride</th>\n",
       "      <th>Desvenlafaxine + Amphetamine</th>\n",
       "      <th>Desvenlafaxine + Amyl Nitrite</th>\n",
       "      <th>Cholecalciferol + Cisapride</th>\n",
       "      <th>Cholecalciferol + Amphetamine</th>\n",
       "      <th>Cholecalciferol + Amyl Nitrite</th>\n",
       "      <th>Cisapride + Amphetamine</th>\n",
       "      <th>Cisapride + Amyl Nitrite</th>\n",
       "      <th>Amphetamine + Amyl Nitrite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aspirin + Abacavir</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.445389</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.640335</td>\n",
       "      <td>0.772584</td>\n",
       "      <td>0.801610</td>\n",
       "      <td>0.683420</td>\n",
       "      <td>0.395382</td>\n",
       "      <td>0.729177</td>\n",
       "      <td>0.734696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182980</td>\n",
       "      <td>0.246319</td>\n",
       "      <td>0.305352</td>\n",
       "      <td>0.095773</td>\n",
       "      <td>0.198760</td>\n",
       "      <td>0.194130</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.284634</td>\n",
       "      <td>0.093931</td>\n",
       "      <td>0.120369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspirin + Acamprosate</th>\n",
       "      <td>0.445389</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.468112</td>\n",
       "      <td>0.613094</td>\n",
       "      <td>0.465688</td>\n",
       "      <td>0.479514</td>\n",
       "      <td>0.458279</td>\n",
       "      <td>0.404861</td>\n",
       "      <td>0.554216</td>\n",
       "      <td>0.281948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079757</td>\n",
       "      <td>0.110612</td>\n",
       "      <td>0.108409</td>\n",
       "      <td>0.114941</td>\n",
       "      <td>0.086620</td>\n",
       "      <td>0.077953</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>0.107962</td>\n",
       "      <td>0.115223</td>\n",
       "      <td>0.083186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspirin + Acarbose</th>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.468112</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.991841</td>\n",
       "      <td>0.898018</td>\n",
       "      <td>0.781938</td>\n",
       "      <td>0.751327</td>\n",
       "      <td>0.560429</td>\n",
       "      <td>0.763201</td>\n",
       "      <td>0.585270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282001</td>\n",
       "      <td>0.287635</td>\n",
       "      <td>0.256554</td>\n",
       "      <td>0.306619</td>\n",
       "      <td>0.283767</td>\n",
       "      <td>0.217588</td>\n",
       "      <td>0.183875</td>\n",
       "      <td>0.247272</td>\n",
       "      <td>0.288974</td>\n",
       "      <td>0.241609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspirin + Acebutolol</th>\n",
       "      <td>0.640335</td>\n",
       "      <td>0.613094</td>\n",
       "      <td>0.991841</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.990330</td>\n",
       "      <td>0.948382</td>\n",
       "      <td>0.925766</td>\n",
       "      <td>0.535153</td>\n",
       "      <td>0.850612</td>\n",
       "      <td>1.254789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366656</td>\n",
       "      <td>0.437728</td>\n",
       "      <td>0.419276</td>\n",
       "      <td>0.235098</td>\n",
       "      <td>0.379933</td>\n",
       "      <td>0.278310</td>\n",
       "      <td>0.159617</td>\n",
       "      <td>0.415895</td>\n",
       "      <td>0.226217</td>\n",
       "      <td>0.180046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspirin + Aceclofenac</th>\n",
       "      <td>0.772584</td>\n",
       "      <td>0.465688</td>\n",
       "      <td>0.898018</td>\n",
       "      <td>0.990330</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.630169</td>\n",
       "      <td>0.900549</td>\n",
       "      <td>0.574863</td>\n",
       "      <td>0.795383</td>\n",
       "      <td>0.692873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226046</td>\n",
       "      <td>0.390838</td>\n",
       "      <td>0.368951</td>\n",
       "      <td>0.181740</td>\n",
       "      <td>0.252595</td>\n",
       "      <td>0.220404</td>\n",
       "      <td>0.114998</td>\n",
       "      <td>0.351631</td>\n",
       "      <td>0.178231</td>\n",
       "      <td>0.146324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cholecalciferol + Amphetamine</th>\n",
       "      <td>0.194130</td>\n",
       "      <td>0.077953</td>\n",
       "      <td>0.217588</td>\n",
       "      <td>0.278310</td>\n",
       "      <td>0.220404</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.196075</td>\n",
       "      <td>0.083083</td>\n",
       "      <td>0.137448</td>\n",
       "      <td>0.301382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932854</td>\n",
       "      <td>0.379479</td>\n",
       "      <td>0.939104</td>\n",
       "      <td>0.245960</td>\n",
       "      <td>0.663581</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.444940</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.163681</td>\n",
       "      <td>0.541008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cholecalciferol + Amyl Nitrite</th>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>0.183875</td>\n",
       "      <td>0.159617</td>\n",
       "      <td>0.114998</td>\n",
       "      <td>0.109826</td>\n",
       "      <td>0.135929</td>\n",
       "      <td>0.069316</td>\n",
       "      <td>0.121056</td>\n",
       "      <td>0.171560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496010</td>\n",
       "      <td>0.220297</td>\n",
       "      <td>0.164338</td>\n",
       "      <td>0.939104</td>\n",
       "      <td>0.576809</td>\n",
       "      <td>0.444940</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.154237</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.597291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cisapride + Amphetamine</th>\n",
       "      <td>0.284634</td>\n",
       "      <td>0.107962</td>\n",
       "      <td>0.247272</td>\n",
       "      <td>0.415895</td>\n",
       "      <td>0.351631</td>\n",
       "      <td>0.363012</td>\n",
       "      <td>0.302794</td>\n",
       "      <td>0.097821</td>\n",
       "      <td>0.242336</td>\n",
       "      <td>0.366744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358707</td>\n",
       "      <td>0.932854</td>\n",
       "      <td>1.257953</td>\n",
       "      <td>0.247396</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.154237</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.444940</td>\n",
       "      <td>0.576809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cisapride + Amyl Nitrite</th>\n",
       "      <td>0.093931</td>\n",
       "      <td>0.115223</td>\n",
       "      <td>0.288974</td>\n",
       "      <td>0.226217</td>\n",
       "      <td>0.178231</td>\n",
       "      <td>0.190175</td>\n",
       "      <td>0.191928</td>\n",
       "      <td>0.091763</td>\n",
       "      <td>0.196902</td>\n",
       "      <td>0.185998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314005</td>\n",
       "      <td>0.496010</td>\n",
       "      <td>0.232432</td>\n",
       "      <td>1.257953</td>\n",
       "      <td>0.541008</td>\n",
       "      <td>0.163681</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.444940</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.663581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amphetamine + Amyl Nitrite</th>\n",
       "      <td>0.120369</td>\n",
       "      <td>0.083186</td>\n",
       "      <td>0.241609</td>\n",
       "      <td>0.180046</td>\n",
       "      <td>0.146324</td>\n",
       "      <td>0.138304</td>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.071006</td>\n",
       "      <td>0.160345</td>\n",
       "      <td>0.161376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220480</td>\n",
       "      <td>0.228122</td>\n",
       "      <td>0.496010</td>\n",
       "      <td>0.932854</td>\n",
       "      <td>0.165082</td>\n",
       "      <td>0.541008</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.576809</td>\n",
       "      <td>0.663581</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1225 rows Ã 1225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Aspirin + Abacavir  Aspirin + Acamprosate   \n",
       "Aspirin + Abacavir                        3.000000               0.445389  \\\n",
       "Aspirin + Acamprosate                     0.445389               3.000000   \n",
       "Aspirin + Acarbose                        0.609524               0.468112   \n",
       "Aspirin + Acebutolol                      0.640335               0.613094   \n",
       "Aspirin + Aceclofenac                     0.772584               0.465688   \n",
       "...                                            ...                    ...   \n",
       "Cholecalciferol + Amphetamine             0.194130               0.077953   \n",
       "Cholecalciferol + Amyl Nitrite            0.077670               0.084203   \n",
       "Cisapride + Amphetamine                   0.284634               0.107962   \n",
       "Cisapride + Amyl Nitrite                  0.093931               0.115223   \n",
       "Amphetamine + Amyl Nitrite                0.120369               0.083186   \n",
       "\n",
       "                                Aspirin + Acarbose  Aspirin + Acebutolol   \n",
       "Aspirin + Abacavir                        0.609524              0.640335  \\\n",
       "Aspirin + Acamprosate                     0.468112              0.613094   \n",
       "Aspirin + Acarbose                        3.000000              0.991841   \n",
       "Aspirin + Acebutolol                      0.991841              3.000000   \n",
       "Aspirin + Aceclofenac                     0.898018              0.990330   \n",
       "...                                            ...                   ...   \n",
       "Cholecalciferol + Amphetamine             0.217588              0.278310   \n",
       "Cholecalciferol + Amyl Nitrite            0.183875              0.159617   \n",
       "Cisapride + Amphetamine                   0.247272              0.415895   \n",
       "Cisapride + Amyl Nitrite                  0.288974              0.226217   \n",
       "Amphetamine + Amyl Nitrite                0.241609              0.180046   \n",
       "\n",
       "                                Aspirin + Aceclofenac  Aspirin + Acemetacin   \n",
       "Aspirin + Abacavir                           0.772584              0.801610  \\\n",
       "Aspirin + Acamprosate                        0.465688              0.479514   \n",
       "Aspirin + Acarbose                           0.898018              0.781938   \n",
       "Aspirin + Acebutolol                         0.990330              0.948382   \n",
       "Aspirin + Aceclofenac                        3.000000              1.630169   \n",
       "...                                               ...                   ...   \n",
       "Cholecalciferol + Amphetamine                0.220404              0.205427   \n",
       "Cholecalciferol + Amyl Nitrite               0.114998              0.109826   \n",
       "Cisapride + Amphetamine                      0.351631              0.363012   \n",
       "Cisapride + Amyl Nitrite                     0.178231              0.190175   \n",
       "Amphetamine + Amyl Nitrite                   0.146324              0.138304   \n",
       "\n",
       "                                Aspirin + Acenocoumarol   \n",
       "Aspirin + Abacavir                             0.683420  \\\n",
       "Aspirin + Acamprosate                          0.458279   \n",
       "Aspirin + Acarbose                             0.751327   \n",
       "Aspirin + Acebutolol                           0.925766   \n",
       "Aspirin + Aceclofenac                          0.900549   \n",
       "...                                                 ...   \n",
       "Cholecalciferol + Amphetamine                  0.196075   \n",
       "Cholecalciferol + Amyl Nitrite                 0.135929   \n",
       "Cisapride + Amphetamine                        0.302794   \n",
       "Cisapride + Amyl Nitrite                       0.191928   \n",
       "Amphetamine + Amyl Nitrite                     0.144597   \n",
       "\n",
       "                                Aspirin + Benziodarone   \n",
       "Aspirin + Abacavir                            0.395382  \\\n",
       "Aspirin + Acamprosate                         0.404861   \n",
       "Aspirin + Acarbose                            0.560429   \n",
       "Aspirin + Acebutolol                          0.535153   \n",
       "Aspirin + Aceclofenac                         0.574863   \n",
       "...                                                ...   \n",
       "Cholecalciferol + Amphetamine                 0.083083   \n",
       "Cholecalciferol + Amyl Nitrite                0.069316   \n",
       "Cisapride + Amphetamine                       0.097821   \n",
       "Cisapride + Amyl Nitrite                      0.091763   \n",
       "Amphetamine + Amyl Nitrite                    0.071006   \n",
       "\n",
       "                                Aspirin + Benzylpenicillin   \n",
       "Aspirin + Abacavir                                0.729177  \\\n",
       "Aspirin + Acamprosate                             0.554216   \n",
       "Aspirin + Acarbose                                0.763201   \n",
       "Aspirin + Acebutolol                              0.850612   \n",
       "Aspirin + Aceclofenac                             0.795383   \n",
       "...                                                    ...   \n",
       "Cholecalciferol + Amphetamine                     0.137448   \n",
       "Cholecalciferol + Amyl Nitrite                    0.121056   \n",
       "Cisapride + Amphetamine                           0.242336   \n",
       "Cisapride + Amyl Nitrite                          0.196902   \n",
       "Amphetamine + Amyl Nitrite                        0.160345   \n",
       "\n",
       "                                Aspirin + Bepridil  ...   \n",
       "Aspirin + Abacavir                        0.734696  ...  \\\n",
       "Aspirin + Acamprosate                     0.281948  ...   \n",
       "Aspirin + Acarbose                        0.585270  ...   \n",
       "Aspirin + Acebutolol                      1.254789  ...   \n",
       "Aspirin + Aceclofenac                     0.692873  ...   \n",
       "...                                            ...  ...   \n",
       "Cholecalciferol + Amphetamine             0.301382  ...   \n",
       "Cholecalciferol + Amyl Nitrite            0.171560  ...   \n",
       "Cisapride + Amphetamine                   0.366744  ...   \n",
       "Cisapride + Amyl Nitrite                  0.185998  ...   \n",
       "Amphetamine + Amyl Nitrite                0.161376  ...   \n",
       "\n",
       "                                Desvenlafaxine + Cholecalciferol   \n",
       "Aspirin + Abacavir                                      0.182980  \\\n",
       "Aspirin + Acamprosate                                   0.079757   \n",
       "Aspirin + Acarbose                                      0.282001   \n",
       "Aspirin + Acebutolol                                    0.366656   \n",
       "Aspirin + Aceclofenac                                   0.226046   \n",
       "...                                                          ...   \n",
       "Cholecalciferol + Amphetamine                           0.932854   \n",
       "Cholecalciferol + Amyl Nitrite                          0.496010   \n",
       "Cisapride + Amphetamine                                 0.358707   \n",
       "Cisapride + Amyl Nitrite                                0.314005   \n",
       "Amphetamine + Amyl Nitrite                              0.220480   \n",
       "\n",
       "                                Desvenlafaxine + Cisapride   \n",
       "Aspirin + Abacavir                                0.246319  \\\n",
       "Aspirin + Acamprosate                             0.110612   \n",
       "Aspirin + Acarbose                                0.287635   \n",
       "Aspirin + Acebutolol                              0.437728   \n",
       "Aspirin + Aceclofenac                             0.390838   \n",
       "...                                                    ...   \n",
       "Cholecalciferol + Amphetamine                     0.379479   \n",
       "Cholecalciferol + Amyl Nitrite                    0.220297   \n",
       "Cisapride + Amphetamine                           0.932854   \n",
       "Cisapride + Amyl Nitrite                          0.496010   \n",
       "Amphetamine + Amyl Nitrite                        0.228122   \n",
       "\n",
       "                                Desvenlafaxine + Amphetamine   \n",
       "Aspirin + Abacavir                                  0.305352  \\\n",
       "Aspirin + Acamprosate                               0.108409   \n",
       "Aspirin + Acarbose                                  0.256554   \n",
       "Aspirin + Acebutolol                                0.419276   \n",
       "Aspirin + Aceclofenac                               0.368951   \n",
       "...                                                      ...   \n",
       "Cholecalciferol + Amphetamine                       0.939104   \n",
       "Cholecalciferol + Amyl Nitrite                      0.164338   \n",
       "Cisapride + Amphetamine                             1.257953   \n",
       "Cisapride + Amyl Nitrite                            0.232432   \n",
       "Amphetamine + Amyl Nitrite                          0.496010   \n",
       "\n",
       "                                Desvenlafaxine + Amyl Nitrite   \n",
       "Aspirin + Abacavir                                   0.095773  \\\n",
       "Aspirin + Acamprosate                                0.114941   \n",
       "Aspirin + Acarbose                                   0.306619   \n",
       "Aspirin + Acebutolol                                 0.235098   \n",
       "Aspirin + Aceclofenac                                0.181740   \n",
       "...                                                       ...   \n",
       "Cholecalciferol + Amphetamine                        0.245960   \n",
       "Cholecalciferol + Amyl Nitrite                       0.939104   \n",
       "Cisapride + Amphetamine                              0.247396   \n",
       "Cisapride + Amyl Nitrite                             1.257953   \n",
       "Amphetamine + Amyl Nitrite                           0.932854   \n",
       "\n",
       "                                Cholecalciferol + Cisapride   \n",
       "Aspirin + Abacavir                                 0.198760  \\\n",
       "Aspirin + Acamprosate                              0.086620   \n",
       "Aspirin + Acarbose                                 0.283767   \n",
       "Aspirin + Acebutolol                               0.379933   \n",
       "Aspirin + Aceclofenac                              0.252595   \n",
       "...                                                     ...   \n",
       "Cholecalciferol + Amphetamine                      0.663581   \n",
       "Cholecalciferol + Amyl Nitrite                     0.576809   \n",
       "Cisapride + Amphetamine                            0.597291   \n",
       "Cisapride + Amyl Nitrite                           0.541008   \n",
       "Amphetamine + Amyl Nitrite                         0.165082   \n",
       "\n",
       "                                Cholecalciferol + Amphetamine   \n",
       "Aspirin + Abacavir                                   0.194130  \\\n",
       "Aspirin + Acamprosate                                0.077953   \n",
       "Aspirin + Acarbose                                   0.217588   \n",
       "Aspirin + Acebutolol                                 0.278310   \n",
       "Aspirin + Aceclofenac                                0.220404   \n",
       "...                                                       ...   \n",
       "Cholecalciferol + Amphetamine                        3.000000   \n",
       "Cholecalciferol + Amyl Nitrite                       0.444940   \n",
       "Cisapride + Amphetamine                              0.803030   \n",
       "Cisapride + Amyl Nitrite                             0.163681   \n",
       "Amphetamine + Amyl Nitrite                           0.541008   \n",
       "\n",
       "                                Cholecalciferol + Amyl Nitrite   \n",
       "Aspirin + Abacavir                                    0.077670  \\\n",
       "Aspirin + Acamprosate                                 0.084203   \n",
       "Aspirin + Acarbose                                    0.183875   \n",
       "Aspirin + Acebutolol                                  0.159617   \n",
       "Aspirin + Aceclofenac                                 0.114998   \n",
       "...                                                        ...   \n",
       "Cholecalciferol + Amphetamine                         0.444940   \n",
       "Cholecalciferol + Amyl Nitrite                        3.000000   \n",
       "Cisapride + Amphetamine                               0.154237   \n",
       "Cisapride + Amyl Nitrite                              0.803030   \n",
       "Amphetamine + Amyl Nitrite                            0.597291   \n",
       "\n",
       "                                Cisapride + Amphetamine   \n",
       "Aspirin + Abacavir                             0.284634  \\\n",
       "Aspirin + Acamprosate                          0.107962   \n",
       "Aspirin + Acarbose                             0.247272   \n",
       "Aspirin + Acebutolol                           0.415895   \n",
       "Aspirin + Aceclofenac                          0.351631   \n",
       "...                                                 ...   \n",
       "Cholecalciferol + Amphetamine                  0.803030   \n",
       "Cholecalciferol + Amyl Nitrite                 0.154237   \n",
       "Cisapride + Amphetamine                        3.000000   \n",
       "Cisapride + Amyl Nitrite                       0.444940   \n",
       "Amphetamine + Amyl Nitrite                     0.576809   \n",
       "\n",
       "                                Cisapride + Amyl Nitrite   \n",
       "Aspirin + Abacavir                              0.093931  \\\n",
       "Aspirin + Acamprosate                           0.115223   \n",
       "Aspirin + Acarbose                              0.288974   \n",
       "Aspirin + Acebutolol                            0.226217   \n",
       "Aspirin + Aceclofenac                           0.178231   \n",
       "...                                                  ...   \n",
       "Cholecalciferol + Amphetamine                   0.163681   \n",
       "Cholecalciferol + Amyl Nitrite                  0.803030   \n",
       "Cisapride + Amphetamine                         0.444940   \n",
       "Cisapride + Amyl Nitrite                        3.000000   \n",
       "Amphetamine + Amyl Nitrite                      0.663581   \n",
       "\n",
       "                                Amphetamine + Amyl Nitrite  \n",
       "Aspirin + Abacavir                                0.120369  \n",
       "Aspirin + Acamprosate                             0.083186  \n",
       "Aspirin + Acarbose                                0.241609  \n",
       "Aspirin + Acebutolol                              0.180046  \n",
       "Aspirin + Aceclofenac                             0.146324  \n",
       "...                                                    ...  \n",
       "Cholecalciferol + Amphetamine                     0.541008  \n",
       "Cholecalciferol + Amyl Nitrite                    0.597291  \n",
       "Cisapride + Amphetamine                           0.576809  \n",
       "Cisapride + Amyl Nitrite                          0.663581  \n",
       "Amphetamine + Amyl Nitrite                        3.000000  \n",
       "\n",
       "[1225 rows x 1225 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_similarity_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and evaluate the SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build the Support Vector Machine. For this, we will use the provided functions from the Scikit-Learn package, a python package used to build different machine learning models. More information can be aquired from the documentation of [__Scikit-Learn__](https://scikit-learn.org/stable/index.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we have to do is split our data into training and testing sets. The parameter *test_size* determines which percentile of our data will be used for testing and which will be used for training. In the code below, we take a tenth of the DDI instances as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs, test_pairs, training_labels, test_labels = train_test_split(\n",
    "    drug_pairs_names, drug_pairs_labels, test_size = 0.1, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our split dataset, we also have to split our similarity matrix into one containing only the combinations of our *training_pairs* and one containing rows of our *test_pairs* and columns of our *training_pairs*.\n",
    "\n",
    "For this we create true copies of the *combined_similarity_matrix* and then remove the rows and columns we do not need respectively.\n",
    "\n",
    "Once this is done, we change the pandas DataFrames into ordinary matrices, removing the labels for columns and rows so we can use them as input into the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the similarity matrix used for training the SVM\n",
    "training_matrix = combined_similarity_matrix.copy()\n",
    "# remove all rows and columns labeled with names of test_set\n",
    "for i in range(len(test_pairs)):\n",
    "    # inplace=True as to not create a copy\n",
    "    # axis=0 drop rows    axis=1 drop columns\n",
    "    training_matrix.drop(test_pairs[i], axis=0, inplace=True) \n",
    "    training_matrix.drop(test_pairs[i], axis=1, inplace=True)\n",
    "\n",
    "# create feature vectors for the prediction of testing data\n",
    "test_matrix = combined_similarity_matrix.copy()\n",
    "# remove all columns labeled with names of test_pairs\n",
    "for i in range(len(test_pairs)):\n",
    "    test_matrix.drop(test_pairs[i], axis=1, inplace=True)\n",
    "# remove all rows labeled with names of training_pairs\n",
    "for i in range(len(training_pairs)):\n",
    "    test_matrix.drop(training_pairs[i], axis=0, inplace=True)\n",
    "\n",
    "# turn pandas DataFrame to ordinary matrix (remove labels)\n",
    "kernel_matrix = training_matrix.to_numpy()\n",
    "test_data = test_matrix.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use a similarity matrix as a kernel function, we have to declare during the creation of the SVM that we will use a precomputed kernel. This is done by setting the parameter *kernel* of the *svm.SVC* function to 'precomputed' instead of keeping it on 'linear' per default. Then, because we use a Soft Margin Classifier, we have to set the parameter *C* which determines the punishment score for missclassifications and outliers.\n",
    "\n",
    "To show the influence of *C* on the model, we create four different SVM, each with a different value for *C*.\n",
    "* classifier_1 : C=0.01\n",
    "* classifier_2 : C=0.1\n",
    "* classifier_3 : C=1.0\n",
    "* classifier_4 : C=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_1 = svm.SVC(kernel='precomputed', C=0.01)\n",
    "classifier_2 = svm.SVC(kernel='precomputed', C=0.1)\n",
    "classifier_3 = svm.SVC(kernel='precomputed', C=1.0)\n",
    "classifier_4 = svm.SVC(kernel='precomputed', C=10.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With setup done, we next train our four SVM with the *fit* function giving them the kernel matrix as the first argument and the list of labels for our training DDI instances as the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10.0, kernel=&#x27;precomputed&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10.0, kernel=&#x27;precomputed&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10.0, kernel='precomputed')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the SVM\n",
    "classifier_1.fit(kernel_matrix,training_labels)\n",
    "classifier_2.fit(kernel_matrix,training_labels)\n",
    "classifier_3.fit(kernel_matrix,training_labels)\n",
    "classifier_4.fit(kernel_matrix,training_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trained SVM, we use our test-data for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier C=0.01:\n",
      "correct_predictions = 72\n",
      "false_predictions = 51\n",
      "Classifier C=0.1:\n",
      "correct_predictions = 66\n",
      "false_predictions = 57\n",
      "Classifier C=1.0:\n",
      "correct_predictions = 64\n",
      "false_predictions = 59\n",
      "Classifier C=10.0:\n",
      "correct_predictions = 59\n",
      "false_predictions = 64\n"
     ]
    }
   ],
   "source": [
    "def predict(classifier, X, y):\n",
    "    \"\"\"\n",
    "    Takes a SVM classifier, test_data X, correct classfification labels y\n",
    "    and predicts the class the test_data belongs to. After that the algorithm\n",
    "    compares the predicted labels to the true labels and counts how many had\n",
    "    been predicted correctly and how many had been predicted wrongly before\n",
    "    outputting them.\n",
    "\n",
    "    params:\n",
    "        classifier = a SVM classifier\n",
    "        X = set of test data to be fed to the SVM classifier for prediction\n",
    "        y = array containing the correct classification labels of X\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = classifier.predict(X)\n",
    "    #print(predictions)\n",
    "    correct_predictions = 0\n",
    "    false_predictions = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == y[i]:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            false_predictions += 1\n",
    "\n",
    "    print(\"correct_predictions = \" + str(correct_predictions))\n",
    "    print(\"false_predictions = \" + str(false_predictions))\n",
    "\n",
    "print(\"Classifier C=0.01:\")\n",
    "predict(classifier_1, test_data, test_labels)\n",
    "print(\"Classifier C=0.1:\")\n",
    "predict(classifier_2, test_data, test_labels)\n",
    "print(\"Classifier C=1.0:\")\n",
    "predict(classifier_3, test_data, test_labels)\n",
    "print(\"Classifier C=10.0:\")\n",
    "predict(classifier_4, test_data, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Wrap up the talktorial's content here and discuss pros/cons and open questions/challenges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created four different SVM for a similarity-based prediction of DDI. Each model took the same input and output data, but differed in the value assigned to the punishment score *C*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Improvements to the algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This talktorial is clearly only an example on how we can use a similarity matrix as a kernel function for predicting DDI and as such has several points where improvements can be made.\n",
    "\n",
    "First of all, the bigger the dataset the more reliable the SVM classifier becomes. We only used 100 drugs here, which certainly creates a large number of possible combinations, but there are far more drugs out on the market that only using 100 of them to train the model leaves us with a substantial lack of information.\n",
    "\n",
    "The next aspect that can be improved - as already mentioned in the Theory part of this talktorial - is which features are chosen and how we handle them computationally. Using more varied properties of drugs to determine similarity can improve the accuracy of predictions, and fingerprints are one of the more basic ways of translating chemical information into data that algorithms can work with. There are other methods as well or various intermediate steps to prepare the input data in advance that can have an effect on the end results. Likewise, the Tanimoto Coefficient is not the only distance metric that can be used to compare between fingerprints and each metric has their own strengths and weaknesses.\n",
    "\n",
    "The similarity matrix in this talktorial is fairly straight-forward. We simply performed matrix addition for the pair-wise similarity matrices of the different features. However, if one wants to put more emphasis on one feature over another the different matrices could be weighted by scalar multiplication before being added to each other. Thus, valuing 3D similarity more than 2D similarity for example.\n",
    "\n",
    "All the above-mentioned improvements are at the discreation of the one setting up the model and depend highly on which input is available and what preferences one has as well as computational capacity.\n",
    "\n",
    "However, an improvement to the above algorithm of the Practical part that should be taken into account is the usage of cross-validation during the fitting process of the SVM classifier. Cross-validation is a good way to prevent overfitting one specific set of training data and allows to see how well the model does overall on different training data. We chose to not show this during this talktorial because we wanted to focus more on the creation and usage of a similarity matrix for SVM modelling rather than on machine modelling etiquette.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "\n",
    "1. Which types of DDI do exist?\n",
    "2. Why should you employ a Soft Margin Classifier for the SVM?\n",
    "3. How can one create a pairwise similarity matrix?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Useful checks at the end</b>: \n",
    "    \n",
    "<ul>\n",
    "<li>Clear output and rerun your complete notebook. Does it finish without errors?</li>\n",
    "<li>Check if your talktorial's runtime is as excepted. If not, try to find out which step(s) take unexpectedly long.</li>\n",
    "<li>Flag code cells with <code># TODO: CI</code> that have deterministic output and should be tested within our Continuous Integration (CI) framework.</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
