{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a34156-d8a3-4f3b-8944-7c1ffa9a6a19",
   "metadata": {},
   "source": [
    "# T001 Â· Scaffold-Based data split\n",
    "Authors:\n",
    "\n",
    "- Vahid Atabaigielmi, CADD seminar 2023, Volkamer lab, Center for Bioinformatics, Saarland University\n",
    "- First and last name, year(s) of contribution, lab, institution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ea113-a65b-467c-b6f6-0afc0e3891d7",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "The aim of this talktorial is to to experiment with different scaffold_based splitting strategies and evaluate the performance of the GCNN model on  test sets to determine the most appropriate splitting approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cfce4-3609-4689-8543-9d39b7931930",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "_Add Table of Contents (TOC) for Theory section._\n",
    "\n",
    "* Information leakage\n",
    "* Molecular scaffold definition\n",
    "* Data spliting methods\n",
    "> * Random based spliting\n",
    "> * Similarity-based Cold-Single Split\n",
    "> * Identity-based Cold-Single Split\n",
    "* Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946661a-3157-4208-8647-3eea424b02b0",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "_Add Table of Contents (TOC) for Practical section._\n",
    "* Import the Data\n",
    "* Random split\n",
    "* Similarity-based Cold-Single Split\n",
    "* Identity-based Cold-Single Split\n",
    "* Plot trainng and test sets\n",
    "* Preprocessing data\n",
    "* Model\n",
    "* Traning\n",
    "* Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79699c8-1b66-4fdf-a034-44011c32c857",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\n",
    "* <i>Journal of cheminformatics</i> (2021), <b>13(1)</b>, 1-14 (https://doi.org/10.1186/s13321-021-00576-2)\n",
    "* <i>pharmacometrics & systems pharmacology</i> (2020), <b>9(3</b>, 129-142 (https://doi.org/10.1002/psp4.12491)\n",
    "* <i>Journal of medicinal chemistry</i> (1996), <b>39(15)</b>, 2887-2893 (https://doi.org/10.1021/jm9602928)\n",
    "* <i>Molecular informatics</i> (2011), <b>30(8)</b>, 646-664 (https://doi.org/10.1002/minf.201100078)\n",
    "* **Talktorial T004** [https://projects.volkamerlab.org/teachopencadd/talktorials/T004_compound_similarity.html]\n",
    "* **Talktorial T037** [https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T037_uncertainty_estimation/talktorial.ipynb]\n",
    "* dataSAIL Document [https://datasail.readthedocs.io/en/latest/index.html]\n",
    "* [https://github.com/kalininalab/DataSAIL/blob/main/docs/index.rst]\n",
    "* ChEMBL web services: [<i>Nucleic Acids Res.</i> (2015), <b>43</b>, 612-620](https://academic.oup.com/nar/article/43/W1/W612/2467881) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69868ff-49ce-40a4-844e-07739cb5f8bf",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cea03e-6009-487d-a7c6-d28812925452",
   "metadata": {},
   "source": [
    "### Information leakage\n",
    "Data leakage is a widespread and critical error that often occurs during the development of machine learning models. The concept of data leakage can be defined in two ways: (i) it happens when information from sources outside the training dataset is utilized to construct the model, or (ii) it occurs when the data used for training a machine learning algorithm coincidentally contains the information being predicted. This leakage of data can have several negative consequences, such as diminishing the model's ability to generalize (overfitting), overestimating the model's performance, or rendering the model entirely invalid. An example of data leakage commonly observed is when data from the test or validation dataset seeps into the training dataset.\n",
    "To avoid data leakage, it is essential to perform data preparation tasks like normalizing variables, engineering features or clustring the data based on their similarety  dataset before spliting  it into training and test sets. The most straightforward approach for splitting data is a random split, however this is not ideal for structure-activity models. The series effect, also known as the series bias or series-dependent bias, refers to a phenomenon commonly observed in quantitative structure-activity relationship (QSAR) modeling. It arises when the data used for training and testing a QSAR model contain chemical compounds that are structurally similar or part of the same chemical series.\n",
    "\n",
    "In QSAR modeling, the goal is to establish a relationship between the structural features (descriptors) of chemical compounds and their corresponding activity or property. However, if the data used for training and testing the model primarily consist of compounds from a few closely related chemical series, it can lead to an inflated model performance during cross-validation but poor generalization to compounds from different series in a prospective setting.\n",
    "\n",
    "The series effect occurs because QSAR models tend to learn and exploit the common structural scaffold patterns within the training data. When the test data contains compounds that are structurally similar to the training compounds, the model may perform well due to the presence of shared descriptors and structural features. However, when the model encounters compounds from different chemical series in real-world applications, it may fail to accurately predict their activities or properties because it hasn't learned the relevant patterns for those series.\n",
    "\n",
    "To mitigate the series effect and evaluate the generalizability of a QSAR model, it is important to carefully design the training and testing data sets. One suggested method, as mentioned is clustring the data according to their similarities and subsequently dividing the clusters into training and test sets. Nevertheless, this approach carries a disadvantage: it can no longer ensure a expected split ratio between the training and test sets. However recently a package for python has been develpoed to address this probelm. DataSAIL is a tool designed to partition data in a manner that minimizes information leakage. This tool approaches dataset splitting as a constrained minimization problem, where it determines the assignment of data points to different partitions by minimizing an objective function that considers the amount of information leakage.\n",
    "The aim of this talktorial is to to experiment with different scaffold_based splitting strategies available in Datasail and evaluate the performance of the feed forward neural network model on  test sets to determine the most appropriate splitting approach. DataSAIL utilizes disciplined quasi-convex programming and binary quadratic programs as its framework for expressing the optimization objective. When it comes to solving such problems, Datasail relies on SCIP, a swift non-commercial solver, as well as MOSEK, a commercial solver that offers complimentary licenses for academic purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8445eaa-ad1c-4ebe-a135-da01a48d9de9",
   "metadata": {},
   "source": [
    "### Molecular Scaffold definition\n",
    "A molecular scaffold refers to the core structure of a molecule, which is composed of its ring systems, linkers, and exocyclic double bonds. It is obtained by removing all terminal side chains from the molecule, while retaining any double bonds that exist in the structure.\n",
    "The concept of a molecular scaffold is particularly relevant in the study of cyclic molecules, where the focus is on the central ring system and its connectivity with other components. By simplifying the structure to its sacffold, researchers can analyze and compare the underlying core structure of different molecules, which can provide insights into their chemical properties and reactivity.\n",
    "It's important to note that the molecular scaffold does not consider the specific substituents or functional groups attached to the rings or linkers. Instead, it provides a simplified representation of the cyclic structure, highlighting its fundamental connectivity and double bond arrangement.\n",
    "\n",
    "![Screenshot%202023-05-08%20at%2021.32.17.png](attachment:Screenshot%202023-05-08%20at%2021.32.17.png)\n",
    "\n",
    "*Figure 1:* \n",
    "Murcko Scaffold: [Bemis, G. W., & Murcko, M. A. (1996). The properties of known drugs. 1. Molecular frameworks. Journal of medicinal chemistry, 39(15), 2887-2893.](https://academic.oup.com/nar/article/43/W1/W612/2467881). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617754d-5fdd-4539-82a7-20827c77d7dc",
   "metadata": {},
   "source": [
    "### Datasail's spliting methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d470e1-bfca-4a08-bfeb-f37eda403dac",
   "metadata": {},
   "source": [
    "#### Random-based splitting\n",
    "This is a simple and commonly used method where the data is randomly divided into subsets. The random splitting can be performed in various ways, such as random assignment of samples to different subsets or randomly shuffling the entire dataset and then dividing it into subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df368215-7d7c-4d64-8d2c-191c1a12acb2",
   "metadata": {},
   "source": [
    "#### Similarity-based Cold-Single Split (CCSe)\n",
    "The process involves dividing a dataset into separate groups or clusters based on the similarity of the data points. This division is done to minimize the sharing of information between the different clusters. The process involves dividing a dataset into separate groups or clusters based on the scaffold similarity of the molecules. This division is done to minimize the sharing of information between the different clusters. The aim is to ensure that the same molecule is not present in more than one cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d5c68-d049-45c7-b906-8143c2417fe9",
   "metadata": {},
   "source": [
    "#### Identity-based Cold-Single Split (ICSe)\n",
    "The dataset is divided into separate splits based on the IDs assigned to each data point. This approach guarantees that every data point is assigned to exactly one split, avoiding any duplication or omission. This becomes particularly advantageous when combined with the weighting of the data points.\n",
    "\n",
    "When weights are assigned to the data points, the splits can be optimized to achieve the desired sizes while considering these weights. This optimization process takes into account the relative importance or significance of each data point, as determined by its weight. By incorporating the weights, the splits can be tailored to meet specific requirements regarding the sizes of the resulting subsets while ensuring that the distribution of important data points is appropriately represented in each split.\n",
    "\n",
    "In summary, splitting the dataset based on data point IDs ensures that every data point appears in only one split, and when combined with weighting, it allows for fine-tuning the splits to meet size requirements while considering the importance of each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12445d-ceb8-4a87-bf64-86e15ffc1180",
   "metadata": {},
   "source": [
    "### Model\n",
    "The feed-forward neural network utilized in this tutorial originates from **Talktorial T037**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9741921-7536-4d33-984a-e6fc3d5061b2",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e59fe9-14e3-4c82-9e05-d1aa4f90c64c",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "914ed393-e3ca-4632-8573-0535265b94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04288557-62d7-44c0-8475-82c16b04f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"qm9_5000.tsv\"\n",
    "#DATA = HERE / \"EGFR_compounds_lipinski.tsv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcfdd4-8068-4271-b462-97a4094ecbba",
   "metadata": {},
   "source": [
    "### Dataset (QM9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4eb9974-c48e-44b1-8d5e-c51a6c01efb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gdb_66703</td>\n",
       "      <td>O=C1[C@H]2[C@@H]3[N@H+]2[C@@H]2[C@H]1[C@]32O</td>\n",
       "      <td>882.3655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gdb_26191</td>\n",
       "      <td>C[C@@H](O)c1cc(N)c[nH]1</td>\n",
       "      <td>1397.2819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gdb_51408</td>\n",
       "      <td>O=CO[C@@H]1[C@@H]2N[C@H]1C2=O</td>\n",
       "      <td>1213.2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gdb_61444</td>\n",
       "      <td>CC(=O)C[C@@H](O)[C@H]1CO1</td>\n",
       "      <td>1448.5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gdb_90638</td>\n",
       "      <td>C[C@H]1N[C@H]2[C@@H]1[C@]2(N)C#N</td>\n",
       "      <td>1226.4068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mol_id                                        smiles         r2\n",
       "0  gdb_66703  O=C1[C@H]2[C@@H]3[N@H+]2[C@@H]2[C@H]1[C@]32O   882.3655\n",
       "1  gdb_26191                       C[C@@H](O)c1cc(N)c[nH]1  1397.2819\n",
       "2  gdb_51408                 O=CO[C@@H]1[C@@H]2N[C@H]1C2=O  1213.2519\n",
       "3  gdb_61444                     CC(=O)C[C@@H](O)[C@H]1CO1  1448.5037\n",
       "4  gdb_90638              C[C@H]1N[C@H]2[C@@H]1[C@]2(N)C#N  1226.4068"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('qm9_5000.tsv',sep='\\t')  # Replace 'qm9.csv' with the path to your dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732cae98-4b5c-441a-b90d-9360635c9efc",
   "metadata": {},
   "source": [
    "### Generate fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc88fa26-9f68-4248-bfba-e663497ee071",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = data['smiles'].values\n",
    "target = data['r2'].values\n",
    "fingerprints = []\n",
    "fingerprint_width = 2048\n",
    "for sm in smiles:\n",
    "    mol = Chem.MolFromSmiles(sm)\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=fingerprint_width)\n",
    "    fingerprints.append(fingerprint)\n",
    "data['fingerprints']=fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c666c-4a65-4944-899c-c3a697995fb4",
   "metadata": {},
   "source": [
    "### Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f425875b-b5fe-41cf-8a74-208ccd4c9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = np.array(fingerprints, dtype=np.float32)\n",
    "\n",
    "fingerprints_train, fingerprints_test, target_train, target_test = train_test_split(fingerprints, target, test_size=0.2, random_state=42)\n",
    "\n",
    "fingerprints_train_RAND = torch.from_numpy(np.array(list(fingerprints_train),dtype=np.float32)).float()\n",
    "fingerprints_test_RAND = torch.from_numpy(np.array(list(fingerprints_test),dtype=np.float32)).float()\n",
    "target_train_RAND = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "target_test_RAND = torch.from_numpy(target_test).float().view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85709ef3-c55a-4dc2-b1f3-b13db034d1dc",
   "metadata": {},
   "source": [
    "### Similarity-based Cold-Single Split (CCSe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8cd7bf9f-e260-498a-851d-09c84d9479c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# molecule_chembl_id\n",
    "cluster_CCSe=pd.read_csv('qm9_5000_CCSe_clusters.tsv', sep='\\t')\n",
    "split_CCSe=pd.read_csv('qm9_5000_CCSe_splits.tsv', sep='\\t')\n",
    "cluster_CCSe.rename(columns={'ID': 'mol_id'},inplace=True)\n",
    "\n",
    "train_CCSe_id=split_CCSe[split_CCSe['Split'].isin(['Split000'])].rename(columns={'ID': 'mol_id'})\n",
    "test_CCSe_id=split_CCSe[split_CCSe['Split'].isin(['Split001'])].rename(columns={'ID': 'mol_id'})\n",
    "\n",
    "train_CCSe_t=data[data['mol_id'].isin(train_CCSe_id['mol_id'])]\n",
    "test_CCSe_t=data[data['mol_id'].isin(test_CCSe_id['mol_id'])]\n",
    "\n",
    "train_CCSe = pd.merge(train_CCSe_t, cluster_CCSe, on='mol_id')\n",
    "test_CCSe = pd.merge(test_CCSe_t, cluster_CCSe, on='mol_id')\n",
    "\n",
    "fingerprints_train_CCSe = torch.from_numpy(np.array(list(train_CCSe['fingerprints']),dtype=np.float32)).float()\n",
    "fingerprints_test_CCSe = torch.from_numpy(np.array(list(test_CCSe['fingerprints']),dtype=np.float32)).float()\n",
    "target_train_CCSe = torch.from_numpy(np.array(train_CCSe['r2'])).float().view(-1, 1)\n",
    "target_test_CCSe = torch.from_numpy(np.array(test_CCSe['r2'])).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a11d4-a96f-4813-9a18-4868af5b5e2a",
   "metadata": {},
   "source": [
    "### Identity-based Cold-Single Split (ICSe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b7545eb-756b-4450-ab93-a5399397a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ICSe=pd.read_csv('qm9_5000_ICSe_splits.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d30e7093-7c48-41a1-af9d-3f585528a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ICSe_id=split_ICSe[split_ICSe['Split'].isin(['Split000'])].rename(columns={'ID': 'mol_id'})\n",
    "test_ICSe_id=split_ICSe[split_ICSe['Split'].isin(['Split001'])].rename(columns={'ID': 'mol_id'})\n",
    "\n",
    "train_ICSe = pd.merge(train_ICSe_id, data, on='mol_id')\n",
    "test_ICSe = pd.merge(test_ICSe_id, data, on='mol_id')\n",
    "fingerprints_train_ICSe = torch.from_numpy(np.array(list(train_ICSe['fingerprints']),dtype=np.float32)).float()\n",
    "fingerprints_test_ICSe = torch.from_numpy(np.array(list(test_ICSe['fingerprints']),dtype=np.float32)).float()\n",
    "target_train_ICSe = torch.from_numpy(np.array(train_ICSe['r2'])).float().view(-1, 1)\n",
    "target_test_ICSe = torch.from_numpy(np.array(test_ICSe['r2'])).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d74ac-f156-42ec-ab2b-533b0a50a3ca",
   "metadata": {},
   "source": [
    "### Feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb5f8319-a0e2-4180-a69f-9f6377e4fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims) - 1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e40f4-97c6-4ae8-aa07-0c46f001974a",
   "metadata": {},
   "source": [
    "Initialize a model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aaf5f0b2-84d2-4156-8f96-f808c6d0e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [2048, 1024, 128, 32]\n",
    "model = FNN(fingerprint_width, hidden_dims, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19908eb2-c80e-437b-a580-d8225cff4f49",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "071d7b7b-3006-4ffa-9009-03e918c01192",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca5d84f6-f10d-47c9-b5e1-d98778c5cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_models(fingerprints_train, fingerprints_test, target_train, target_test, split_method):\n",
    "    num_epochs = 20\n",
    "    batch_size = 32\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        indices = torch.randperm(fingerprints_train.size(0))\n",
    "        fingerprints_train = fingerprints_train[indices]\n",
    "        target_train = target_train[indices]\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i in range(0, fingerprints_train.size(0), batch_size):\n",
    "            batch_fingerprints = fingerprints_train[i:i+batch_size]\n",
    "            batch_target = target_train[i:i+batch_size]\n",
    "\n",
    "            outputs = model(batch_fingerprints)\n",
    "\n",
    "            loss = criterion(outputs, batch_target)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_losses.append(epoch_loss.item())\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_outputs = model(fingerprints_test)\n",
    "            test_loss = criterion(test_outputs, target_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "            #print(f'Epoch [{epoch+1}/{num_epochs}], Training loss: {epoch_loss.item()}, Test loss: {test_loss.item()}')\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.plot(train_losses, label='Training loss')\n",
    "    ax1.set_title(f'{split_method} Training loss over epochs for dataset of size {len(fingerprints_train)+len(fingerprints_test)}')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(test_losses, label='Test loss')\n",
    "    ax2.set_title(f'{split_method} Test loss over epochs for dataset of size {len(fingerprints_train)+len(fingerprints_test)} ')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'loss_plot_{split_method}_{len(fingerprints_train)+len(fingerprints_test)}.png')  # Save the figure to a file\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1e5f2-d791-400e-b3fe-f9a5c70783f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models(fingerprints_train_RAND, fingerprints_test_RAND, target_train_RAND, target_test_RAND, 'random split')\n",
    "training_models(fingerprints_train_CCSe, fingerprints_test_CCSe, target_train_CCSe, target_test_CCSe, 'CCSe split')\n",
    "training_models(fingerprints_train_ICSe, fingerprints_test_ICSe, target_train_ICSe, target_test_ICSe, 'ICSe split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262df03f-fc83-4653-b5fa-3480b39fb394",
   "metadata": {},
   "source": [
    "## Assess test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa56d7-b45d-45fe-a1bc-2234e5b3f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation(fingerprints_test, target_test, title, ax, color, dataset):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_outputs = model(fingerprints_test)\n",
    "        test_loss = criterion(test_outputs, target_test)\n",
    "    predictions = test_outputs.numpy()\n",
    "\n",
    "    # Calculate additional evaluation metrics\n",
    "    mse = mean_squared_error(target_test.flatten(), predictions.flatten())\n",
    "    mae = mean_absolute_error(target_test.flatten(), predictions.flatten())\n",
    "\n",
    "    ax.scatter(predictions.flatten(), target_test.flatten(), s=3, alpha=0.5, color=color)\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Truth')\n",
    "    ax.grid(True, linestyle=':', linewidth=0.5)\n",
    "\n",
    "    # Display additional evaluation metrics\n",
    "    ax.text(0.03, 0.97, f'MSE: {mse:.4f}', transform=ax.transAxes)\n",
    "    ax.text(0.03, 0.93, f'MAE: {mae:.4f}', transform=ax.transAxes)\n",
    "    ax.set_title(f'{title} split method over {len(dataset)} data points', loc='center')\n",
    "\n",
    "    # Remove the legend warning\n",
    "    ax.legend([])\n",
    "\n",
    "    # Return evaluation metrics as a dictionary\n",
    "    return {\n",
    "        'Split method': title,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'Test Set Size': round(len(fingerprints_test) / len(data), 2)\n",
    "    }\n",
    "\n",
    "# Control number of plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 6), sharey=True)\n",
    "\n",
    "colors = ['green', 'blue', 'orange', 'red', 'purple', 'brown']\n",
    "\n",
    "# Call evaluation for each dataset\n",
    "results = []\n",
    "results.append(evaluation(fingerprints_test_RAND, target_test_RAND, \"RAND\", axs[0], colors[0], data))\n",
    "results.append(evaluation(fingerprints_test_CCSe, target_test_CCSe, \"CCSe\", axs[1], colors[1], data))\n",
    "results.append(evaluation(fingerprints_test_ICSe, target_test_ICSe, \"ICSe\", axs[2], colors[2], data))\n",
    "\n",
    "# Print results as a table\n",
    "table = tabulate(results, headers='keys', tablefmt='fancy_grid')\n",
    "print(f'Table: Evaluation Results')\n",
    "print(table)\n",
    "\n",
    "# Save the table as a text file\n",
    "with open('evaluation_results.txt', 'w') as file:\n",
    "    file.write(f'Table: Evaluation Results\\n')\n",
    "    file.write(table)\n",
    "\n",
    "# Save the plots\n",
    "for result, ax in zip(results, axs):\n",
    "    title = result['Split method']\n",
    "    plt.savefig(f'results_{len(data)}.png')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
