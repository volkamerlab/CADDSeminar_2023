{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Thank you for contributing to TeachOpenCADD!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Set up your PR</b>: Please check out our <a href=\"https://github.com/volkamerlab/teachopencadd/issues/41\">issue</a> on how to set up a PR for new talktorials, including standard checks and TODOs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# · Diffusion-based docking models\n",
    "\n",
    "**Note:** This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Hamza Ibrahim, CADD seminars 2023, Universität des Saarlandes (UdS)\n",
    "- Michael Bockenköhler, 2023,  [Volkamer lab](https://volkamerlab.org), Universität des Saarlandes (UdS)\n",
    "- Andrea Volkamer, 2023,  [Volkamer lab](https://volkamerlab.org), Universität des Saarlandes (UdS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "This talktorial presents two state-of-the-art classes of generative models. You will learn what generative models are and know the basics of two powerful classes of generative models. We explore then their potential application in  molecular docking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "* Generative models\n",
    "    * Denoising diffusion probabilistic model (DDPM).\n",
    "        1. Forward process\n",
    "        2. Reverse process\n",
    "        3. DDPM training\n",
    "            - Loss function\n",
    "            - Network architecture\n",
    "    * Score-based generative model\n",
    "        1. Score model training\n",
    "        2. Score model with stochastic differential equations (SDEs)\n",
    "\n",
    "* Diffusion-based docking models.\n",
    "    1. Ligand pose manifold\n",
    "    2. Product space diffusion\n",
    "    3. Model architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "* Data preparation.\n",
    "    - Download PDB structure\n",
    "    - Prepare input file\n",
    "* DiffDock implementation\n",
    "* Denoising visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Score-based generative modeling through stochastic differential equations: [<i>arXiv</i> (2021)](https://arxiv.org/pdf/2011.13456.pdf) \n",
    "* Equivariant Graph Neural Networks: [<i>arXiv</i> (2022)](https://arxiv.org/pdf/2102.09844.pdf)\n",
    "* Structure-based Drug Design with Equivariant Diffusion Models: [<i>arXiv</i> (2022)](https://arxiv.org/pdf/2210.13695.pdf)\n",
    "* DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking: [<i>arXiv</i> (2023)](https://arxiv.org/pdf/2210.01776v2.pdf)\n",
    "* [Diffusion Model Clearly Explained!](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)\n",
    "* Deep Unsupervised Learning using Nonequilibrium Thermodynamics: [Sohl-Dickstein et al. <i>arXiv</i> (2021)](https://arxiv.org/pdf/1503.03585.pdf)\n",
    "* Generative Modeling by Estimating Gradients of the Data Distribution: [Song et al. <i>arXiv</i> (2019)](https://arxiv.org/abs/1907.05600)\n",
    "* Denoising Diffusion Probabilistic Models [Ho et al. <i>arXiv</i> (2020)](https://arxiv.org/abs/2006.11239)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models are a category of machine learning models that have the capability to generate new data by learning the data distribution of a given training data set by injecting noise to the input. In a nutshell ***\"Creating noise from data is easy; creating data from noise is generative modeling.\"*** [Song et al 2021](https://arxiv.org/abs/2011.13456).\n",
    "\n",
    "In this section, we are going to discuss two advanced techniques used in generative modelling,denoising diffusion probabilistic model and score-based generative models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising diffusion probabilistic model (DDPM)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPM or so called \"diffusion model\" is inspired from Physics by non-equilibrium thermodynamics[DDPM paper]. It learns to generate new data depending on two main reciprocal processes that represent two sets of random variables organized in the form of Markov chains."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forward Diffusion Process → add noise to input data.\n",
    "2. Reverse Diffusion Process → denoise noised data.\n",
    "\n",
    "![DGM processes figure](images/basics_dgm.png)\n",
    "\n",
    "*Figure 1:* \n",
    "Black arrows represent the forward diffusion process, while blue arrow represents the reverse diffusion process\n",
    "Figure is taken from: [Medium article](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Forward process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first process adds guassian noise sequentially to the input data $x_0$ by $T$ steps. As $T → \\infty $, $x_T$ becomes a complete static noise image as in figure [1]. So every successive state \n",
    "$\\mathbb{x}_{t + 1}$ could be computed as the following : \n",
    "$$\n",
    "q(\\mathbb{x}_{t}|{x}_{t-1}) = \\mathcal{N(\\mathbb{x}_{t};\\mathbb{\\mu}_t = \\sqrt{1 - \\beta}{x}_{t-1}, \\Sigma_t = \\beta_t \\mathbf{I})}, \\tag{1}\n",
    "$$\n",
    "Where $q(\\mathbb{x}_{t}|{x}_{t-1})$ denotes the distribution of the next state $\\mathbb{x}_{t}$.\n",
    "\n",
    " $\\mathbb{\\mu}_t$ and $\\Sigma_t({x}_t, t)$ represent the mean and covariance of next state distribution, respectively.\n",
    "\n",
    "Utilizing [Reparametrization Trick](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166#228f) closed-form formula could be derived, which prompts us to sample ${x}_{t}$ at any time step using ${x}_{0}$. It makes forward diffusion process much faster as following:\n",
    "$$\n",
    "{x}_{t} = \\sqrt{{\\bar{\\alpha}}_t} {x}_0 + \\sqrt{1 - {\\bar\\alpha}_t} {\\epsilon}_0, \\tag{2}\n",
    "$$\n",
    "\n",
    "Where ${\\bar\\alpha}_t = \\prod_{s = 0}^{t}{1 - {\\beta}_s}$ , and ${\\epsilon}_0, ... , {\\epsilon}_{t-2}, {\\epsilon}_{t-1} \\sim \\mathcal{N (0 , \\mathbf{I})}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Reverse process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunetaly, It's not possible to sample ${x}_{0}$ from ${x}_{t}$ using $q(\\mathbb{x}_{t-1}|{x}_{t})$ as in forward process, because reversing the noise is intractable, therefore **reverse diffusion process** is employed. As a solution $q(\\mathbb{x}_{t-1}|{x}_{t})$ could be approximated by using a deep learning model (e.g. neural network), which predicts an approximation to the conditional probability distribution $\\mathbb{p}_{\\theta}(\\mathbb{x}_{t-1}|{x}_{t})$, which modeled as a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\mathbb{p}_{\\theta}(\\mathbb{x}_{t-1}|{x}_{t}) = \\mathcal{N(\\mathbb{x}_{t-1};\\mathbb{\\mu}_\\theta({x}_t, t), \\Sigma_t({x}_t, t))}, \\tag{3}\n",
    "$$\n",
    "\n",
    "By learning the conditional probability densities using deep learning model the original image $x_0$ is reconstructed from the noisy image $\\mathbb{x}_t$ as illustrated in _figure 1_. Allowing for the extraction of meaningful information from the noisy representation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After explaining the two main processes of diffusion models, we start now with training the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train a diffusion model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of training a diffusion model is to learn data distribution of the input data from the noised version. \n",
    "\n",
    "In order to effectively train a generative model, It's necessary to define an optimized loss function and the architecture of the deep learning model. In this section we will explain briefly the loss function and the network archietecture, which commonly employed in diffusion models then we'll explain the training process of diffusion models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illusterated before, diffusion model has some similarity with variational autoencoders (VAEs). They are both generative models used to learn data distribution to generate new data. Mximizing the log-liklihood guides the model towards capturing patterns and statistical properties in noised data. \n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{\\text{max}}\\sum_{i=1}^{N}\\log{p_\\theta(x_i)} \\tag{4}\n",
    "$$\n",
    "\n",
    "In diffusion models the log likelihood is intractable. However, we can indirectly optimize it by optimizing the lower variational bound. By skipping mathimatical details, [Ho et al. (2020)](https://arxiv.org/pdf/2006.11239.pdf) has simplified the loss function to:\n",
    "$$\n",
    "{L}_{t}^{simple} = \\mathbb{E}_{t \\sim [1,T] ,x_0, \\epsilon}[|| \\epsilon - {\\epsilon}_\\theta(\\sqrt{\\bar{a}} x_0 + \\sqrt{1 - \\bar{a}} \\epsilon,t)||^2] \\tag{5}\n",
    "$$ \n",
    "where: \n",
    "\n",
    "$\\epsilon \\sim \\mathcal{N}(0, \\mathbb{I})$ is the actual noise added, whch follows a standard normal distribution.\n",
    "\n",
    "${\\epsilon}_\\theta(\\sqrt{\\bar\\alpha} x_0 + \\sqrt{1 - \\bar\\alpha} \\epsilon,t) = {\\epsilon}_\\theta(x_t,t) $ denotes the approximated noise from neural network using reparamarization trick that mentioned before.\n",
    "\n",
    "In case of DGMs the true value corresponds to the distribution of added noise that introduced to an image and the model's objective is to learn the original data distribution from the added noise on the inp.\n",
    "Usually the loss function is the difference between predicted and true values.  As observed, the loss function is the mean square error (MSE) of the added noise and predicted noise.\n",
    "\n",
    "Once the loss function has been chosen, we can go to the next step, which is selecting an appropriate network architecture and training the diffusion model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Network architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important requirment of the network is to have the identical dimensionality for the input and the output. Therefore, usually [U-Net](https://theaisummer.com/unet-architectures/) is commonly used for prediction tasks in DGMs as a network architecture.\n",
    "\n",
    "The U-Net architecture based on an encoder-decoder structure. In encoding, the spatial dimensions decreases while the number of channels increases keeping the important features of the input data. On the contrary, in decoding the spatial dimensions increase while number of channels decrease to produce the same spatial dimensions as the input data as illustrated in Figure 2.\n",
    "\n",
    "\n",
    "<img src=\"images/Unet-architecture.png\"  style=\"margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "*Figure 2:* \n",
    "An overview of U-Net architecture.\n",
    "Figure is taken from: [AI Summer](https://theaisummer.com/static/fa507fda71846a516801bccb19474aec/0012b/Unet-architecture.png)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By optimizing the gradient descent of the loss function, the model can be trained until it converges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got a clear idea of the outline of DDPM. In the next section, we'll explain another type of generative models, which is score-based generative models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score-based generative model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a given data set {$x_1, ..., x_{N-1}, x_N$} which follows a certain probability distribution, denoted as $p(x)$. The primary goal of the score model is to fit a model to the given distribution $p(x)$ so that new data points can be generated by sampling from the learned distribution.\n",
    "\n",
    "However, first we need to think of a way to represent the probability distribution. One way is to model probability density function (PDF) directly. So, let $f_\\theta(x) \\isin \\mathbb{R}$ parameterized by $\\theta$, which is learnable parameter. PDF of a probabilistic model $f_\\theta(x)$ is defined as follow : \n",
    "\n",
    "$$\n",
    "p_\\theta(x)= \\frac{e^{-f_\\theta(x)}}{\\mathbb{Z_\\theta}}, \\tag{6}\n",
    "$$\n",
    "\n",
    "As $\\mathbb{Z_\\theta} > 0$ is the normalizing constant, depend on $\\theta$. The model can be trained by maximizing the log-likelihood of the PDF as in equation (4).\n",
    "\n",
    "However, a general normalizing constant $\\mathbb{Z_0}$ is intractable and we can't compute $p_\\theta(x)$. To overcome the intractibility problem of $\\mathbb{Z_0}$, **score function** is modelled instead of the PDF, which is defined as :\n",
    "\n",
    "$$\n",
    "s_\\theta(x) = \\nabla_x \\log {p(x)} \\tag{7}\n",
    "$$\n",
    "\n",
    "$s_\\theta(x)$ is score-based model and can be parametrized without the need to evaluate $\\mathbb{Z_0}$. By taking the gradient of the distribution, the normalizing constant becomes zero and we can ignore it as in equation 8.\n",
    "\n",
    "$$\n",
    "s_\\theta(x) = - \\nabla_x f_\\theta(x) - \\nabla_x \\log{\\mathbb{Z_0}} =  - \\nabla_x f_\\theta(x) \\tag{8}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in figure 3, there is no need to use normalization while parameterizing score functions. On the contrary, changes in data distribution undergo normalization, as the area under the curve (AUC) must integrate to one.\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/ebm.gif\" height=\"280\" width=\"400\" />\n",
    "  <img src=\"images/score.gif\" height=\"280\" width=\"400\" /> \n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 3:* \n",
    "Parameterization of probability density functions (on the left) and score functions (on left). \n",
    "Figures are taken from: [Yang-Song blogpost](https://yang-song.net/blog/2021/score/#connection-to-diffusion-models-and-others)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Score model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a score-based model, we need to compare between the model and the actual data distribution. This is done by minimizing a function that computes the distance between ground-truth data score and score-based model like Fisher divergence, which defined as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E_{p(x)}}[|| \\nabla_x \\log{p_{(x)}} - \\mathbb{s_\\theta(x)}||_2^2] \\tag{9}\n",
    "$$\n",
    "\n",
    "The ground-truth data score is unknown, which makes it infeasible to compute fisher divergence indirectly. However, **score matching** makes it feasible, because it can minimize fisher divergence without the estimation of the ground-truth data score and allow us to train the model.\n",
    "\n",
    "The model can be trained, but the main objective of the score-based model, which is generating new data, is not achieved. [Langevin dynamics](https://en.wikipedia.org/wiki/Langevin_dynamics) is used as a sampling method to generate new data, accessing only its score function as shown in figure 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/smld.jpg\"  style=\"margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "*Figure 4:* \n",
    "An overview of score-based generative modeling with score matching and langevin dynamics.\n",
    "Figure is taken from: [Yang-Song blogpost](https://yang-song.net/blog/2021/score/#connection-to-diffusion-models-and-others)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Score model with stochastic differential equations (SDEs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding multiple noise scale at different scales has shown an improvement to the model's ability to generate high quality samples. By increasing the noise scale to infinity, exact log-likelihood can be also obtained. By injecting noise to the data, the concept becomes similar to DDPM. However, the main difference in this scenario is that the noise perturbation is a continous-time [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process#:~:text=A%20stochastic%20process%20is%20defined,measurable%20with%20respect%20to%20some) as shown in figure 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sde_schematic.jpeg\" height=400 width =1000  style=\"margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "*Figure 5:* \n",
    "An overview of a forward and reverse SDE in general.\n",
    "Figure is taken from: [Yang-Song blogpost](https://yang-song.net/blog/2021/score/#connection-to-diffusion-models-and-others)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like DDPM we get now forward process and reverse process of SDE. Keep in mind that using SDE is not a unique approach and there are different ways to add noise perturbations, one way is showed in the example. However, Score function is used to generate data in the reverse SDE, as demonstrated in figure 5. which will use score matching for training the score-based model. For more more comprehensive understanding, [This article](https://yang-song.net/blog/2021/score/#connection-to-diffusion-models-and-others) explains every tiny detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, two main state-of-the-art generative models are well-covered. In the next section, we discuss a novel tool called _DiffDock_. It has employed a generative model to tackle the challenge of molecular docking in the field of cheminformatics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion-based docking model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main concepts of creating a generative models are explained. It's important to note that implementing generative model in molecular docking will not be the same as we explained. In this section we discuss the challenges encountered in applying generative model, especially score-based generative model, in molecular docking and how these obstacles have been overcome in a real-case application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ligand pose manifold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have diffusion-based docking model, you have to think of a manifold that suits ligand poses first where $L \\isin \\mathbb{R}^{3n} $ as $n$ is the number of atoms. If we start forward diffusing without setting any limitations for the degree of freedom, it becomes absurd and ligands will have unreasonable bond lengths and angles as in figure 6.\n",
    "\n",
    "<img src=\"images/absurd_ligand.png\"  style=\"margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "*Figure 6:* \n",
    "Randomizing bond length and angles without keeping local structures fixed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution to this problem is presented in [DiffDock paper](https://arxiv.org/pdf/2210.01776v2.pdf). They are inspired from traditional docking approches by taking already embedded ligand in a 3D space using RDKit , which instantiates the angles and bond length of the atoms. Instead of thinking of a ligand as an element in an eucledian space, they described ligand pose by four main parameters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Local structures like bond lengths, bond angles, chirality and ring structure are generated using RDKit and kept fixed in order to maintain integrity of the predicted ligand and the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Position of ligand with 3D translastion group were left flexible to find the pocket and fit in it $\\mathbb{R}^3$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Rotation parameterization, where $Rotation \\isin {SO(3)}$ correspnds to 3D rigid rotation around the mass centre of the ligand.\n",
    "\n",
    "<img src=\"images/rotation.gif\"  style=\"width:300px;\">\n",
    "\n",
    "\n",
    "*Figure 7:* \n",
    "GIF shows an example of the rotaion of a methyl group in an Ethane structure. Figure taken from [Proteopedia](https://proteopedia.org/wiki/index.php/Dihedral/Index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Flexibility of torsion angles to fit in the pocket, where: $ \\mathit{Torsions} \\isin \\mathbb{T}^m$, which represent the changes in torsion angles around rotatable bonds in a ligand with a copy of 2D rotation group ${SO(2)}$ .  \n",
    "\n",
    "<img src=\"images/Phipsi-AH.gif\"  style=\"width:400px;\">\n",
    "\n",
    "*Figure 8:* \n",
    "GIF illustrartes the torsion angles and changes in it. As shown a torsion angle ϕ is defined by  a four covalently bonded atoms. Every three atoms defines a half plane and when these planes intersect the angle between them is torsion angle ϕ. Figure taken from [Proteopedia](https://proteopedia.org/wiki/index.php/Dihedral/Index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four parameters have introduced a new challenge. The problem arises from the fact that there are several valid possibilities for making changes through rotations and alterations in torsion angles together. The used strategy in DiffDock is to _disentangle_ the degrees of freedom involved in docking, which aims to isolate the modifcation of torsion angles from other transformations such as rotation and translations.\n",
    "\n",
    "To make sure that the changes in torsion were totally independent during docking, post-torsion RMSD alignment was performed to confirm that rotations and translations were orthogonal to torsion modifications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By utilizing those parameters, it was possible to map ligand poses into submanifold $\\mathcal{M}_c \\subset \\mathbb{R}^{3n}$, where they can easily diffuse over. This submanifold $\\mathcal{M}_c$ facilitates diffusion over a space where ligand poses are represented in $(m + 6)$ dimensions, where $m$ denotes number of rotatable bonds.\n",
    "\n",
    "Fortunately, the ligand pose submanifold establishes a smooth mapping with the product space. As a result, we can now map displacements within the manifold of ligand poses to the product space. This product space, denoted as : $\\mathbb{P} = \\mathbb{R}^3 * \\isin {SO(3)} * \\isin \\mathbb{T}^m$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Product space diffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After mapping ligand pose manifold to product space, score-based generative model with SDE is trained with **score matching** according to [Song et al. 2019](https://arxiv.org/abs/1907.05600) to compute the score of the diffusion kernel on the product space and sample from it. But here appears another problem which is how the score model will be diffused on the product space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that most of existing score-based generative models are designed for a data on an eucalidean space. However, [De Bortoli et al. 2022](https://arxiv.org/pdf/2202.02763.pdf) has developed Riemannian score-based generative model (SGM) which based on Riemannian manifold which gives the possibility to create SGMs of a various manifolds.\n",
    "\n",
    "The main concept is to consider the score model not as a vector field on the eucledian space, but rather as a vector field on the manifold where score and the score model are elements of the tangent space of every possible point on the manifold as represented in _figure 8_. \n",
    "\n",
    "<img src=\"images/tangent_space.png\"  style=\"width:400px;\">\n",
    "\n",
    "*Figure 8:* The tangent space, denoted as ${T_xM}$ represents the set of all possible tangent vectors $v$ at $x$ as $x \\isin \\mathcal{M}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As mentioned before product space is the product of three manifolds. So, in order to proceed the forward diffusion process on the product space, every manifold will be diffused independently according to [Rodol`a et al., 2019](https://arxiv.org/abs/1809.10940) and the tangent space will become a direct sum of every manifold:\n",
    "\n",
    "$$\n",
    "T_g \\mathbb{P} = T_r\\mathbb{T^3} \\oplus T_RSO(3) \\oplus T_\\theta SO(2)^m \n",
    "$$\n",
    "Therefore, we can sample from diffusion kernel and perform regression independently against its true score within each group."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence model, besides a score model, are constructed using [E(3)NN](https://arxiv.org/abs/2207.09453). Score-based generative model is used to simulate the reverse diffusion starting from the \"noisy\" version of the ligand-portein interaction using reverse SDE to denoising and find the right binding pocket\n",
    "\n",
    "While confidence model is responisble for ranking the arbitrary generated number of ligands. It's trained as a classifier specially to rank the poses and find best generated conformers as demonestrated in figure 10."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ChEMBL web service schema](images/DiffDock.png)\n",
    "\n",
    "*Figure 10:* \n",
    "Overview of DiffDock workflow. Left: The model takes as input the separate ligand and protein\n",
    "structures. Center: Score-based generative model where random initial poses are denoised via a reverse SDE over trans-\n",
    "lational, rotational, and torsional degrees of freedom. Right:. The sampled poses are ranked by the\n",
    "confidence model to produce a final prediction and confidence score.\n",
    "Figure and discription taken from: [arXiv 2023](https://arxiv.org/pdf/2210.01776v2.pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiffDock has shown an significant improvement in comparison to tradition docking. It achieved a $38%%$% success rate in making predictions with RMSD below $2$, whereas the best traditional docking tool used got $28%$%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data preparation.\n",
    "    - Download PDB structure\n",
    "    - Prepare input file\n",
    "* DiffDock implementation\n",
    "* Denoising visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the practical part we are going to implement _DiffDock_, which used in molecular docking trained using score-based generative model. It's now open-source and published on [Github](https://github.com/gcorso/DiffDock)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os \n",
    "#import nglview as nv\n",
    "import urllib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting _DiffDock_ implementation, input data has to be prepared. We start with the protein structure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare protein structure\n",
    "\n",
    "If you want to download the protein structure. Set `protein_pdb` to your PDB code of the structure you want to use. You can use your own protein, but you need to place it inside data/ directory and set protein_pdb to your protein pdb structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, it's set to apixaban-bound ABLE crystal structure\n",
    "\n",
    "protein_pdb = '6w70.pdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDB structure already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download the PDB structure if it's not in DATA\n",
    "\n",
    "if protein_pdb not in os.listdir('data'):\n",
    "    urllib.request.urlretrieve(f'http://files.rcsb.org/download/{protein_pdb}', f'data/{protein_pdb}')\n",
    "else:\n",
    "    print('PDB structure already downloaded.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify your quary ligand\n",
    "\n",
    "In this step, you need to specify your ligand input. There are different input types can be used. SDF file or SMILES string. In case you want to use sdf file, change `ligand` variable to the path of SDF file.\n",
    "\n",
    "Unlike _GNINA_, _DiffDock_ use one quary per protein. Therefore, if you want to dock more than one ligand to the same protein, you can add it to `ligand_input` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give ligand SMILES here\n",
    "ligand = \"O=C(O)c1cc(/OCc2cccc3ccccc23)ccc1O\"\n",
    "\n",
    "molecule_id = ['Molecule_1']\n",
    "ligand_input = [ligand]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiffDock implementation\n",
    "\n",
    "First step is to download _DiffDock_ software from its [Github repository](https://github.com/gcorso/DiffDock.git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffDock is alreay cloned.\n"
     ]
    }
   ],
   "source": [
    "if \"DiffDock\" not in os.listdir(HERE):\n",
    "    #specifiy version\n",
    "    !git clone https://github.com/gcorso/DiffDock.git\n",
    "else:\n",
    "    print(f\"DiffDock is alreay cloned.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cloning the repository, you can create anaconda environment for DiffDock using the next cell, in order to avoid any running issues.\n",
    "\n",
    "Note: If you didn't install anaconda on your system, more information can be found [here](https://www.anaconda.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: conda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: conda\n"
     ]
    }
   ],
   "source": [
    "!conda env create -f DiffDock/environment.yml\n",
    "!conda activate diffdock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure your docking settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hamza/Desktop/Bioinformatics_master/SS23/CADDSeminar_2023/notebook/T02_DiffusionBasedDocking/data/6w70.pdb\n"
     ]
    }
   ],
   "source": [
    "samples_per_complex = \"5\"\n",
    "inference_steps = \"10\"\n",
    "actual_steps = \"18\"\n",
    "batch_size = \"5\"\n",
    "print(f'{DATA}/{protein_pdb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ESM language model embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/mambaforge-pypy3/envs/diffdock/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6003.\n",
      "  warnings.warn(\n",
      "/home/hamza/mambaforge-pypy3/envs/diffdock/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 6128.\n",
      "  warnings.warn(\n",
      "/home/hamza/mambaforge-pypy3/envs/diffdock/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6221.\n",
      "  warnings.warn(\n",
      "/home/hamza/mambaforge-pypy3/envs/diffdock/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 6381.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 1 batches (2 sequences)\n",
      "HAPPENING | confidence model uses different type of graphs than the score model. Loading (or creating if not existing) the data for the confidence model now.\n",
      "Size of test dataset:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on ['1a0q'] index 10 is out of bounds for axis 0 with size 10\n",
      "Failed for 1 complexes <torch_geometric.loader.dataloader.DataLoader object at 0x7fbc0b37c160>\n",
      "Skipped 0 complexes\n",
      "Results are in /home/hamza/Desktop/Bioinformatics_master/SS23/CADDSeminar_2023/notebook/T02_DiffusionBasedDocking/data/Molecule_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:05, 65.16s/it]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"DiffDock/\")\n",
    "for id, smiles in zip(molecule_id, ligand_input):\n",
    "    diffdock_cmd = f\"python -m inference --protein_path {DATA}/{protein_pdb} --ligand '{smiles}' --out_dir {DATA}/{id} --inference_steps {inference_steps} --samples_per_complex {samples_per_complex} --save_visualisation --batch_size {batch_size} --actual_steps {actual_steps} --no_final_step_noise\"\n",
    "    os.system(diffdock_cmd)\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidURL",
     "evalue": "URL can't contain control characters. '/pdb/files//Users/hamzaibrahim/Github/CADDSeminar_2023/notebook/T02_DiffusionBasedDocking/data/<built-in function id>/rank1_reverseprocess.cif' (found at least ' ')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnglview\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnv\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m view \u001b[39m=\u001b[39m nv\u001b[39m.\u001b[39;49mshow_pdbid(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mDATA\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mid\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/rank1_reverseprocess\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# Or use nv.show_file(\"path/to/your/file.pdb\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Display the molecular viewer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m display(view)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/site-packages/nglview/show.py:54\u001b[0m, in \u001b[0;36mshow_pdbid\u001b[0;34m(pdbid, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Show PDB entry.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[39mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m>>> w # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     53\u001b[0m structure \u001b[39m=\u001b[39m PdbIdStructure(pdbid)\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m NGLWidget(structure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/site-packages/nglview/widget.py:245\u001b[0m, in \u001b[0;36mNGLWidget.__init__\u001b[0;34m(self, structure, representations, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m structure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_structure(structure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m representations:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# If initial representations are provided,\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39m# we need to set defaultRepresentation to False\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentations \u001b[39m=\u001b[39m representations\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/site-packages/nglview/widget.py:1124\u001b[0m, in \u001b[0;36mNGLWidget.add_structure\u001b[0;34m(self, structure, **kwargs)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(structure, Structure):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mstructure\u001b[39m}\u001b[39;00m\u001b[39m is not an instance of Structure\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_data(structure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ngl_component_ids\u001b[39m.\u001b[39mappend(structure\u001b[39m.\u001b[39mid)\n\u001b[1;32m   1126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/site-packages/nglview/widget.py:1238\u001b[0m, in \u001b[0;36mNGLWidget._load_data\u001b[0;34m(self, obj, **kwargs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_url:\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39mget_structure_string\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m-> 1238\u001b[0m         blob \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mget_structure_string()\n\u001b[1;32m   1239\u001b[0m         kwargs2[\u001b[39m'\u001b[39m\u001b[39mext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mext\n\u001b[1;32m   1240\u001b[0m         passing_buffer \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/site-packages/nglview/adaptor.py:125\u001b[0m, in \u001b[0;36mPdbIdStructure.get_structure_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_structure_string\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    124\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttp://www.rcsb.org/pdb/files/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdbid \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.cif\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m urlopen(url)\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    522\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    524\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 525\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    527\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    528\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/urllib/request.py:542\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    541\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 542\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    543\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    544\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    545\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/urllib/request.py:1383\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPConnection, req)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/urllib/request.py:1354\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1354\u001b[0m         h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1355\u001b[0m                   encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1356\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[1;32m   1254\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1255\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/http/client.py:1267\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maccept-encoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m header_names:\n\u001b[1;32m   1265\u001b[0m     skips[\u001b[39m'\u001b[39m\u001b[39mskip_accept_encoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1267\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mputrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mskips)\n\u001b[1;32m   1269\u001b[0m \u001b[39m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \u001b[39m# the caller passes encode_chunked=True or the following\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m \u001b[39m# conditions hold:\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[39m# 1. content-length has not been explicitly set\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[39m# 2. the body is a file or iterable, but not a str or bytes-like\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[39m# 3. Transfer-Encoding has NOT been explicitly set by the caller\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m header_names:\n\u001b[1;32m   1277\u001b[0m     \u001b[39m# only chunk body if not explicitly set for backwards\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# compatibility, assuming the client code is already handling the\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# chunking\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/http/client.py:1101\u001b[0m, in \u001b[0;36mHTTPConnection.putrequest\u001b[0;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_method \u001b[39m=\u001b[39m method\n\u001b[1;32m   1100\u001b[0m url \u001b[39m=\u001b[39m url \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_path(url)\n\u001b[1;32m   1103\u001b[0m request \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (method, url, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn_str)\n\u001b[1;32m   1105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_request(request))\n",
      "File \u001b[0;32m~/micromamba/envs/dockm82/lib/python3.8/http/client.py:1201\u001b[0m, in \u001b[0;36mHTTPConnection._validate_path\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   1199\u001b[0m match \u001b[39m=\u001b[39m _contains_disallowed_url_pchar_re\u001b[39m.\u001b[39msearch(url)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m match:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mURL can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain control characters. \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1202\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(found at least \u001b[39m\u001b[39m{\u001b[39;00mmatch\u001b[39m.\u001b[39mgroup()\u001b[39m!r}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidURL\u001b[0m: URL can't contain control characters. '/pdb/files//Users/hamzaibrahim/Github/CADDSeminar_2023/notebook/T02_DiffusionBasedDocking/data/<built-in function id>/rank1_reverseprocess.cif' (found at least ' ')"
     ]
    }
   ],
   "source": [
    "import nglview as nv\n",
    "\n",
    "view = nv.show_pdbid(f\"{DATA}/{id}/rank1_reverseprocess\")  # Or use nv.show_file(\"path/to/your/file.pdb\")\n",
    "\n",
    "# Display the molecular viewer\n",
    "display(view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'py3Dmol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpy3Dmol\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrdkit\u001b[39;00m \u001b[39mimport\u001b[39;00m Chem\n\u001b[1;32m      4\u001b[0m \u001b[39m# Read the PDB file\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'py3Dmol'"
     ]
    }
   ],
   "source": [
    "import py3Dmol\n",
    "from rdkit import Chem\n",
    "\n",
    "# Read the PDB file\n",
    "with open(f'{DATA}/{protein_pdb}', 'r') as pdb_file:\n",
    "    pdb_data = pdb_file.read()\n",
    "\n",
    "# Read the SDF file\n",
    "suppl = Chem.SDMolSupplier(f'{DATA}/results/Molecule_1/1a0q/rank1.sdf')\n",
    "print(suppl)\n",
    "# Create a viewer\n",
    "viewer = py3Dmol.view(width=800, height=600)\n",
    "\n",
    "# Add the PDB data to the viewer\n",
    "viewer.addModel(pdb_data, 'pdb')\n",
    "\n",
    "# Iterate over the molecules in the SDF file\n",
    "for mol in suppl:\n",
    "    # Skip invalid molecules\n",
    "\n",
    "    if mol is None:\n",
    "        continue\n",
    "\n",
    "    # Add the SDF molecule to the viewer\n",
    "    viewer.addModel(Chem.MolToMolBlock(mol), 'sdf')\n",
    "\n",
    "# Set the style and visualization options\n",
    "# viewer.addModel(Chem.MolToMolBlock(suppl), 'sdf')\n",
    "viewer.setStyle({'cartoon': {'color': 'spectrum'}})\n",
    "viewer.zoomTo()\n",
    "\n",
    "# Display the viewer\n",
    "viewer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Throughout this talktorial, the fundementals of two main types of generative models are explained, highlighting their powerful capibility to generate new data from using minimal information as an input. We've presented a case study in the field of cheminformatics and how it's used in molecular docking. While some problems have been solved, new questions raised that need to be addressed. Proving that molecular docking remains a vital field that offers room for more improvement using robust and well-structured models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Ask three questions that the user should be able to answer after doing this talktorial. Choose important take-aways from this talktorial for your questions.\n",
    "\n",
    "1. From presenting two main types of generative models, address their benefits and differences between them.\n",
    "2. What is the unit of the predicted DiffDock output? In comparison to traditional molecular docking tools, is it any better to get this output? and why?\n",
    "3. What are the limitations of DiffDock? and can it be overcome in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
